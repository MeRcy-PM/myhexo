[{"title":"TCP的FRTO分析","date":"2017-04-11T11:04:12.000Z","path":"2017/04/11/TCP的FRTO分析/","text":"TCP的FRTO理解本文主要描述内核4.9.4中的TCP丢包处理与frto相关的操作，主要覆盖使用sack的场景。 0. 参考文档[1] RFC-5682[2] linuxtcp.ps[3] frto.pdf 1. FRTO要解决的问题FRTO主要是用来处理在DSACK生效时，突发的延迟触发RTO超时后，不必要的延迟和重传报文的ack造成了DSACK而产生非必要的快速重传[1]。传统的基于DSACK的RTO超时会有如下问题: 1) 在16.5s和17s中间最后一个ack到来之前一直处于慢启动阶段，指数发送数据， 2) 18s虚假RTO触发，其实只是延迟，但是这时候RTO工作，重传丢失的报文。 3) 19s的时候延迟包的ack到达，注意到此时ack的数据序列号是大于重传报文的序列号。 4) 延迟的ack使发送端继续重传之后延迟的ack之后的数据(这部分数据之前已经发送过))(19s到19.5s之间的重传)。 5) 在19.8s，之前发送的最大序列号报文被确认，接着由于4中的重传报文陆续到达，接收端发送了一系列以最大序列号报文为ack的Duplicate SACK。 6) 发送端收到这些重复的DSACK后，触发了快速重传，降低了传输性能。 使用FRTO后的效果: 1) 同上 2) 同上 3) 同上 4) 由于延迟包的ack更新了snd_una，因此这里不重传其余数据，而是发送两个新的分片。 5) 延迟包的ack陆续到达，此时由于未重传的包收到了对应的ack，因此可以判断当前是一个虚假的RTO，继续发送新的数据。 1. FRTO rfc解释这里主要针对sack的场景，即rfc-5682的section 3。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758591) When the retransmission timer expires, retransmit the first unacknowledged segment and set SpuriousRecovery to FALSE. Following the recommendation in the SACK specification [MMFR96], reset the SACK scoreboard. If &quot;RecoveryPoint&quot; is larger than or equal to SND.UNA, do not enter step 2 of this algorithm. Instead, set variable &quot;RecoveryPoint&quot; to indicate the highest sequence number transmitted so far and continue with slow-start retransmissions following the conventional RTO recovery algorithm.2) Wait until the acknowledgment of the data retransmitted due to the timeout arrives at the sender. If duplicate ACKs arrive before the cumulative acknowledgment for retransmitted data, adjust the scoreboard according to the incoming SACK information. Stay in step 2 and wait for the next new acknowledgment. If the retransmission timeout expires again, go to step 1 of the algorithm. When a new acknowledgment arrives, set variable &quot;RecoveryPoint&quot; to indicate the highest sequence number transmitted so far. a) If the Cumulative Acknowledgment field covers &quot;RecoveryPoint&quot; but not more than &quot;RecoveryPoint&quot;, revert to the conventional RTO recovery and set the congestion window to no more than 2 * MSS, like a regular TCP would do. Do not enter step 3 of this algorithm. b) Else, if the Cumulative Acknowledgment field does not cover &quot;RecoveryPoint&quot; but is larger than SND.UNA, transmit up to two new (previously unsent) segments and proceed to step 3. If the TCP sender is not able to transmit any previously unsent data -- either due to receiver window limitation or because it does not have any new data to send -- the recommended action is to refrain from entering step 3 of this algorithm. Rather, continue with slow-start retransmissions following the conventional RTO recovery algorithm. It is also possible to apply some of the alternatives for handling window-limited cases discussed in Appendix A.3) The next acknowledgment arrives at the sender. Either a duplicate ACK or a new cumulative ACK (advancing the window) applies in this step. Other types of ACKs are ignored without any action. a) If the Cumulative Acknowledgment field or the SACK information covers more than &quot;RecoveryPoint&quot;, set the congestion window to no more than 3 * MSS and proceed with the conventional RTO recovery, retransmitting unacknowledged segments. Take this branch also when the acknowledgment is a duplicate ACK and it does not acknowledge any new, previously unacknowledged data below &quot;RecoveryPoint&quot; in the SACK information. Leave SpuriousRecovery set to FALSE. b) If the Cumulative Acknowledgment field or a SACK information in the ACK does not cover more than &quot;RecoveryPoint&quot; AND it acknowledges data that was not acknowledged earlier (either with cumulative acknowledgment or using SACK information), declare the timeout spurious and set SpuriousRecovery to SPUR_TO. The retransmission timeout can be declared spurious, because the segment acknowledged with this ACK was transmitted before the timeout. 注1: RecoveryPoint为tcp中的high_seq，后称恢复点。注2: snd_nxt为下一个将要发送的包的包号。 3.1: 当重传定时器超时了，首先重传第一个未确认的分片，同时设置SpuriousRecovery为false，并重置sack的计分板。如果恢复点大于等于下一个要发的包(好像基本不可能，最多等于)，则将恢复点设置为当前发送的最大包。进入常规恢复。否则进入step 2。 3.2: 等待重传数据的ack到来。如果重复的ack比新的ack更早到达，则更新相应的sack计分板，同时留在第二步，等待新的ack到来。如果重传定时器再次超时，回到第一步。当新的ack到来后，更新恢复点。 a) 如果到来的ack中包括了恢复点，但不超过恢复点，撤销至普通恢复，同时拥塞窗口设置不大于2倍MSS。不进入第三步。 b) 如果到来的ack中不包含恢复点，但是收到的包超过未确认包，发送两个新的分片，并进入步骤3。如果当前发送端无法发送新报文(接收窗口限制或者没有新的应用层数据)，这里建议不要进入步骤3，而是使用恢复步骤。-3.3: 下一个ack到来，除了新的ack或者是重复ack，其他ack在这个步骤中都被忽略。 a) 如果新的ack包含了恢复点，则设置拥塞窗口不超过3倍MSS大小并执行恢复操作，重传未确认分片。这个分支也处理重复ack但是没有确认任何新块的场景。 b) 如果新的ack或者sack信息中没有包含恢复点，且确认的数据是之前没有确认的，则认为这个超时是一个奇怪的超时，并设置SpuriousRecovery为SPUR_TO。这个重传被标记为奇怪的，因为这个分片在超时前被确认了。 其实论文中的图比rfc这段(恢复点的几个判断理解费力。。)更好理解，也更契合代码，为了识别出虚假的RTO： 超时后先只重发丢失的一个包(3.1)。 判断重传后的第一个新的ack是否更新了snd_una，如果更新了snd_una，就发送新的数据，在判断虚假RTO之前不重传数据(3.2.b) 如果没重传就已经收到数据，尝试撤销本次RTO，如果撤销成功，这就是一个虚假RTO，进入恢复流程(3.3.b)。如果还是重复ack，则认为这是一个丢包事件(3.3.a)。 2. 4.9.4中的代码这里主要分析tcp_process_loss函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/* Process an ACK in CA_Loss state. Move to CA_Open if lost data are * recovered or spurious. Otherwise retransmits more on partial ACKs. */// 这个函数中如果可以恢复，则会直接进入CA_OPEN。// 不进入CA_OPEN则会直接返回tcp_ack中进行重传(或者是发送新分片)。static void tcp_process_loss(struct sock *sk, int flag, bool is_dupack, int *rexmit)&#123; struct tcp_sock *tp = tcp_sk(sk); // 完全恢复 bool recovered = !before(tp-&gt;snd_una, tp-&gt;high_seq); // 最大的未确认包已经发生改变，可能是之前丢包已经恢复，尝试撤销丢包状态。 // 当frto关闭时，这是唯二可以退出LOSS状态的条件。 if ((flag &amp; FLAG_SND_UNA_ADVANCED) &amp;&amp; tcp_try_undo_loss(sk, false)) return; // frto在enter_loss时候置位，根据超时时的设置决定是否可能是一个虚假的RTO。 if (tp-&gt;frto) &#123; /* F-RTO RFC5682 sec 3.1 (sack enhanced version). */ /* Step 3.b. A timeout is spurious if not all data are * lost, i.e., never-retransmitted data are (s)acked. */ // #define FLAG_ORIG_SACK_ACKED 0x200 /* Never retransmitted data are (s)acked */ // 包没有丢，因为还没重传就收到了ACK，只是延迟了，这是虚假RTO，撤销丢包状态。 // 注: 这里的frto_undo传参为true，必定恢复。 // 对应RTO论文图中的延迟包的acks。 if ((flag &amp; FLAG_ORIG_SACK_ACKED) &amp;&amp; tcp_try_undo_loss(sk, true)) return; if (after(tp-&gt;snd_nxt, tp-&gt;high_seq)) &#123; // 如果是刚进入LOSS状态，会先尝试重传，这时候snd_nxt总是等于high_seq的， // 这个分支主要对应3.2.b之后发送了两个新分片。 // 虽然发送了新分片，没有发重传，但是这时候收到的ack并没有更新una // 说明这个rtt中，之前una的包仍旧没有达到，因此这里认为他是真的超时 // 关闭frto。对应3.3.a if (flag &amp; FLAG_DATA_SACKED || is_dupack) tp-&gt;frto = 0; /* Step 3.a. loss was real */ &#125; else if (flag &amp; FLAG_SND_UNA_ADVANCED &amp;&amp; !recovered) &#123; // 这里进入的条件为 // 1. snd_nxt == high_seq，还没发送过新分片 // 2. una更新过，且没有完全恢复 // 执行3.2.b，发送新分片。 // 对应论文图中收到了一个更新过snd_una的ack。 tp-&gt;high_seq = tp-&gt;snd_nxt; /* Step 2.b. Try send new data (but deferred until cwnd * is updated in tcp_ack()). Otherwise fall back to * the conventional recovery. */ // 3.2.b中判断当前是否可以发送新分片。 if (tcp_send_head(sk) &amp;&amp; after(tcp_wnd_end(tp), tp-&gt;snd_nxt)) &#123; *rexmit = REXMIT_NEW; return; &#125; tp-&gt;frto = 0; &#125; &#125; // 已经完全恢复，则撤销对应的恢复操作，并进入TCP_CA_OPEN状态。后续将进入恢复状态。 // 这里主要处理了其他几个不在FRTO可处理的场景，如3.2.a和3.3.a // 唯一进入这里但frto还可能生效的场景为: // 发送新分片后，但是收到了一个不是新的sack，且不是一个dup sack。 // 在这种情况下的处理应该和上一个旧的sack相同。 // 个人理解应该是3.3中被忽略的case。 if (recovered) &#123; /* F-RTO RFC5682 sec 3.1 step 2.a and 1st part of step 3.a */ tcp_try_undo_recovery(sk); return; &#125; if (tcp_is_reno(tp)) &#123; /* A Reno DUPACK means new data in F-RTO step 2.b above are * delivered. Lower inflight to clock out (re)tranmissions. */ if (after(tp-&gt;snd_nxt, tp-&gt;high_seq) &amp;&amp; is_dupack) tcp_add_reno_sack(sk); else if (flag &amp; FLAG_SND_UNA_ADVANCED) tcp_reset_reno_sack(tp); &#125; *rexmit = REXMIT_LOST;&#125; 3. 简单流程图 3.1 常规frto流程 1) RTO超时，发送新分片 2) 收到一个ack，进入tcp_process_loss处理，此时frto开启，如果延迟的ack更新了una，则直接恢复loss状态。如果刚好等于重传包，这时候先发送新分片。 3) 又一个新的ack到来，这时候由于没有重传包，延迟的ack会更新una，直接撤销丢包处理，离开LOSS状态。 3.2 frto恢复失败流程 1) RTO超时，发送新分片 2) 收到一个ack，进入tcp_process_loss处理，此时frto开启，如上发送新分片。 3) 又一个新的ack到来，这时候由于丢包，不更新对应的una，因此关闭frto，进入重传流程。","tags":[{"name":"TCP","slug":"TCP","permalink":"http://yoursite.com/tags/TCP/"},{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"拥塞控制","slug":"拥塞控制","permalink":"http://yoursite.com/tags/拥塞控制/"}]},{"title":"小怼的世界","date":"2017-04-06T13:56:41.000Z","path":"2017/04/06/hello-world/","text":"先打(怼)个(下)招(涛)呼(哥)吧小刘，出来接客了……. 为什么要写博客我：涛哥我考你一考，你有博客吗涛：不能写罢？……我教给你，记着！这些博客应该记着。将来做架构师的时候，写ppt要用。我：谁要你教，不就写博客么？涛：对呀对呀！……博客有四种写法，你知道么？我愈不耐烦了，努着嘴走远。涛哥见我毫不热心，便又叹一口气，显出极惋惜的样子。 真Verson 2.0-beta建立个人知识体系 初看一些东西，会觉得这个东西可能很简单，但是被人问及后，可能会在很多细节上出现很多漏洞。写博客可以在细节上进行思考，完善自己的知识体系。 写些什么呢 想到啥写啥吧","tags":[]},{"title":"QUIC翻译","date":"2017-04-02T02:57:50.000Z","path":"2017/04/02/QUIC翻译/","text":"QUIC中文翻译翻译地址 翻译过程一些心路历程: 距离上次完整翻译一篇文章已经好几年了，那还是本科毕设用的文章翻译，那时候还能用谷歌翻译，现在也又能了………. 词汇量还是太差。。 对于翻译，并不用把每句话都翻译出来，整段话意思一致就可以了。 对于某些名词，也不需要完完全全翻译。","tags":[{"name":"QUIC","slug":"QUIC","permalink":"http://yoursite.com/tags/QUIC/"},{"name":"传输协议","slug":"传输协议","permalink":"http://yoursite.com/tags/传输协议/"}]},{"title":"linux内核慢启动拥塞避免代码分析","date":"2017-03-27T02:27:29.000Z","path":"2017/03/27/linux内核慢启动拥塞避免代码分析/","text":"@(Network)[tcp, 慢启动] TCP拥塞控制两个速率增长阶段分析0. 参考文档[1] rfc-5681[2] tcp-abc-rfc[3] rfc-3465[4] rfc-3742 1. 拥塞控制个人理解1.1 慢启动与拥塞避免慢启动和拥塞避免，主要是用于拥塞控制中拥塞窗口增长的维护。 根据阈值，拥塞控制其实分为两部分，小于阈值的慢启动阶段，大于阈值进入拥塞避免阶段。 慢启动作为拥塞控制的一部分，我觉得其名字取的比较具有混淆性。个人理解的慢启动分为两种，一种是拥塞窗口小于阈值时候正常的一个指数增长的过程，这个过程中的拥塞窗口不会重置，会持续增长，还有一种是与快速恢复对应的慢启动重新启动，这种时候会将拥塞窗口重置为1，并重新开始指数增长。这么理解的原因如下： 在文档中描述快速恢复时，当收到三个重复ack时候，这时候可能并不是实际丢包，可能是因为链路问题，较晚到达接收端。 在BSD 4.3之前会进入慢启动阶段，但是理论上慢启动一般是指数上升的过程，反而是拥塞避免阶段线性上升速度较慢，且拥塞避免会更新当前的拥塞窗口和阈值，会出现小范围衰减。 假如tcp认为当前包丢失，会很严格的重置拥塞窗口(具体代码如rto触发tcp_enter_loss)，这时候速率曲线不会只是单纯减低到某个值，而是会降低到零点。 1.2 快速恢复和快速重传因此，个人理解，老版本上收到三个重复ack认为丢包，进入丢包处理，重置了拥塞窗口，在非重复ack到来后，拥塞窗口仍然需要从零开始指数上升，而对于快速恢复而言，其只进入拥塞避免阶段，拥塞窗口只是进行一定修正，在非重复ack到来后，仍然能根据阈值来决定是否执行非重启的慢启动，这时候恢复速度相较于严格的丢包处理快了不少。 由于tcp对于丢包的容忍极低，一旦丢包发生，就会进入严格的拥塞处理，而RTO是丢包主要判断依据，因此快速重传也是针对tcp对于丢包容忍度低的一个修正，避免进入RTO，直接影响传输性能。 2. 拥塞控制代码分析本章主要基于reno的拥塞控制。下文中的代码均基于linux kernel 2.6.32版本，直到linux kernel 4.9-rc8之前的版本，tcp整体并没有太大变化。本文不分析frto相关内容。 2.1 调用链由于文档描述上是直接给出一个计算过程，如慢启动阶段的指数上升，和代码直观上看略有不同，因此这里需要先缕清楚整个的调用链，能更好的描述整个拥塞控制的过程。 tcp的拥塞主要是基于定时器(RTO)和ack的，因此主要处理函数都以tcp_ack为起点。这里不分析整个tcp_ack函数，仅分析常规调用链。 整体入口如下： 1234567891011121314151617// 当ack时一个可疑的ack，如sack，或者路由发送的显示拥塞控制，或者当前拥塞状态不是正常状态时。if (tcp_ack_is_dubious(sk, flag)) &#123; /* Advance CWND, if state allows this. */ if ((flag &amp; FLAG_DATA_ACKED) &amp;&amp; !frto_cwnd &amp;&amp; tcp_may_raise_cwnd(sk, flag)) // 当窗口仍然满足可以增长的条件时，进入拥塞控制， // 这是一个钩子函数，具体实现由具体拥塞控制算法来实现， // 对于reno而言可能是慢启动，可能是拥塞避免。 tcp_cong_avoid(sk, ack, prior_in_flight); // 处理拥塞状态机，暂时不展开 tcp_fastretrans_alert(sk, prior_packets - tp-&gt;packets_out, flag);&#125; else &#123; // 当这个ack是一个正常的数据确认包，进入拥塞控制 if ((flag &amp; FLAG_DATA_ACKED) &amp;&amp; !frto_cwnd) tcp_cong_avoid(sk, ack, prior_in_flight);&#125; 2.2 tcp reno的拥塞控制tcp reno注册到拥塞控制框架中的是tcp_reno_cong_avoid函数。 其代码较为简单，只是其中多了一部分tcp-abc的拥塞避免算法，其慢启动实现在tcp_slow_start中，可以参考[rfc-3465][tcp_abc]。大体是用已经确认的byte大小来作为拥塞控制的计算，在慢启动阶段会更加激进，但是可能会带来更大的burst。 1234567891011121314151617181920212223242526272829303132333435/* * TCP Reno congestion control * This is special case used for fallback as well. *//* This is Jacobson&apos;s slow start and congestion avoidance. * SIGCOMM &apos;88, p. 328. */void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 in_flight)&#123; struct tcp_sock *tp = tcp_sk(sk); if (!tcp_is_cwnd_limited(sk, in_flight)) return; /* In &quot;safe&quot; area, increase. */ // 小于阈值会进入慢启动环节，不重置窗口的慢启动。 if (tp-&gt;snd_cwnd &lt;= tp-&gt;snd_ssthresh) tcp_slow_start(tp); /* In dangerous area, increase slowly. */ else if (sysctl_tcp_abc) &#123; /* RFC3465: Appropriate Byte Count * increase once for each full cwnd acked */ // RFC3465的拥塞避免算法，使用bytes_acked来作为修改拥塞窗口的判断条件 if (tp-&gt;bytes_acked &gt;= tp-&gt;snd_cwnd*tp-&gt;mss_cache) &#123; tp-&gt;bytes_acked -= tp-&gt;snd_cwnd*tp-&gt;mss_cache; if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp) tp-&gt;snd_cwnd++; &#125; &#125; else &#123; // 拥塞避免 tcp_cong_avoid_ai(tp, tp-&gt;snd_cwnd); &#125;&#125; 2.3 慢启动慢启动里面额外涉及两篇rfc，rfc-3742和tcp_abc。 其中snd_cwnd_cnt为线性增长器，只有当线性增长器大于一个窗口大小时，其才会将发送窗口增加，即其单位为1/snd_cwnd，后续还会在拥塞避免代码中见到。 刚开始看代码时对下面那个循环并不是很理解，不理解为什么++是指数增长，直到放到整个调用栈上看，其具体流程如代码注释中所写，为指数增长的过程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/* * Slow start is used when congestion window is less than slow start * threshold. This version implements the basic RFC2581 version * and optionally supports: * RFC3742 Limited Slow Start - growth limited to max_ssthresh * RFC3465 Appropriate Byte Counting - growth limited by bytes acknowledged */void tcp_slow_start(struct tcp_sock *tp)&#123; int cnt; /* increase in packets */ /* RFC3465: ABC Slow start * Increase only after a full MSS of bytes is acked * * TCP sender SHOULD increase cwnd by the number of * previously unacknowledged bytes ACKed by each incoming * acknowledgment, provided the increase is not more than L */ // 不满足tcp abc的窗口增加条件，此时确认的字节数小于mss_cache。 if (sysctl_tcp_abc &amp;&amp; tp-&gt;bytes_acked &lt; tp-&gt;mss_cache) return; // RFC 3742，限制慢启动在一个RTT内的burst。 if (sysctl_tcp_max_ssthresh &gt; 0 &amp;&amp; tp-&gt;snd_cwnd &gt; sysctl_tcp_max_ssthresh) cnt = sysctl_tcp_max_ssthresh &gt;&gt; 1; /* limited slow start */ else // 加上一个窗口大小，在没有abc的情况，保证在最底下的循环中拥塞窗口大小至少增加1. cnt = tp-&gt;snd_cwnd; /* exponential increase */ /* RFC3465: ABC * We MAY increase by 2 if discovered delayed ack */ // tcp-abc，慢启动阶段更激进的burst。 if (sysctl_tcp_abc &gt; 1 &amp;&amp; tp-&gt;bytes_acked &gt;= 2*tp-&gt;mss_cache) cnt &lt;&lt;= 1; tp-&gt;bytes_acked = 0; // 更新snd_cwnd_cnt(窗口线性增长器) tp-&gt;snd_cwnd_cnt += cnt; // 线性增长器是窗口的多少倍，窗口就增加多少。 // 注意：这里的标准场景下的线性增长，每次也只增长1个窗口大小， // 但是其仍然是指数增长，因此每个窗口发出去的数据对应一个ack， // 而每一个ack都会对应触发一次增长。 // 以下为一个简单的例子，sender为发送端，receiver为接收端 // px为包号为x的包，ack x为对第x个包的确认 // snd_cwnd为拥塞窗口 // sender receiver // p1 (snd_cwnd 1) ---------------------------&gt; // // &lt;--------------------------- ack 1 // snd_cwnd++ (2) // // p2 (snd_cwnd 2) ---------------------------&gt; // p3 (snd_cwnd 2) ---------------------------&gt; // // &lt;--------------------------- ack 2 // snd_cwnd++ (3) // &lt;--------------------------- ack 3 // snd_cwnd++ (4) // // p4 (snd_cwnd 4) ---------------------------&gt; // p5 (snd_cwnd 4) ---------------------------&gt; // p6 (snd_cwnd 4) ---------------------------&gt; // p7 (snd_cwnd 4) ---------------------------&gt; // // &lt;--------------------------- ack 4 // snd_cwnd++ (5) // &lt;--------------------------- ack 5 // snd_cwnd++ (6) // &lt;--------------------------- ack 6 // snd_cwnd++ (7) // &lt;--------------------------- ack 7 // snd_cwnd++ (8) // send with snd_cwnd = 8 (p8 - p15) // 每一个ack对应增加一个窗口大小，不丢包的场景下相当于窗口以指数上升 // 1 --&gt; 2 --&gt; 4 --&gt; 8 while (tp-&gt;snd_cwnd_cnt &gt;= tp-&gt;snd_cwnd) &#123; tp-&gt;snd_cwnd_cnt -= tp-&gt;snd_cwnd; if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp) tp-&gt;snd_cwnd++; &#125;&#125; 2.4 拥塞避免拥塞避免的代码比较简短，注意2.3中所写的，snd_cwnd_cnt为线性增长器，其单位为1 / w。在reno调用中，这里的w也为snd_cwnd窗口大小。即每一个ack只增加1 / snd_\\cwnd大小的窗口。 123456789101112/* In theory this is tp-&gt;snd_cwnd += 1 / tp-&gt;snd_cwnd (or alternative w) */void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w)&#123; // 每次cnt++，直到w次后snd_cwnd++，即单位 1 / w if (tp-&gt;snd_cwnd_cnt &gt;= w) &#123; if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp) tp-&gt;snd_cwnd++; tp-&gt;snd_cwnd_cnt = 0; &#125; else &#123; tp-&gt;snd_cwnd_cnt++; &#125;&#125; 3. kernel 4.9的改变对tcp_slow_start的改动不算是4.9的，早在3.18之前就已经改变了，使用的已经不是之前的snd_cwnd_cnt，而是采用tcp-abc算法来进行慢启动。 慢启动仍然使用类似tcp-abc的实现机制，不过其并不以byte作为单位，而是以MSS作为单位进行处理。1234567891011121314151617181920212223/* Slow start is used when congestion window is no greater than the slow start * threshold. We base on RFC2581 and also handle stretch ACKs properly. * We do not implement RFC3465 Appropriate Byte Counting (ABC) per se but * something better;) a packet is only considered (s)acked in its entirety to * defend the ACK attacks described in the RFC. Slow start processes a stretch * ACK of degree N as if N acks of degree 1 are received back to back except * ABC caps N to 2. Slow start exits when cwnd grows over ssthresh and * returns the leftover acks to adjust cwnd in congestion avoidance mode. */u32 tcp_slow_start(struct tcp_sock *tp, u32 acked)&#123; // 使用确认的包数(其中可能包括sack的确认，或者重传数据的确认都加上) // 来更新窗口值，而不是之前的byte。 // 在函数tcp_clean_rtx_queue中有更新对应的delivered。 // 其更新的值貌似和MSS有关系。 u32 cwnd = min(tp-&gt;snd_cwnd + acked, tp-&gt;snd_ssthresh); // 当acked仍然有值，说明超过阈值，处理完slow start后还会进行congestion avoid的处理。 acked -= cwnd - tp-&gt;snd_cwnd; tp-&gt;snd_cwnd = min(cwnd, tp-&gt;snd_cwnd_clamp); return acked;&#125; 拥塞避免上和老版本类似，也使用到了线性增长器，但是涨幅比之前版本较大，并不是以1为计数，而是以acked，即已经确认的MSS个数据片作为单位。 12345678910111213141516171819202122/* In theory this is tp-&gt;snd_cwnd += 1 / tp-&gt;snd_cwnd (or alternative w), * for every packet that was ACKed. */void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w, u32 acked)&#123; /* If credits accumulated at a higher w, apply them gently now. */ // 第一次线性增长计算。 if (tp-&gt;snd_cwnd_cnt &gt;= w) &#123; tp-&gt;snd_cwnd_cnt = 0; tp-&gt;snd_cwnd++; &#125; // 以 acked / snd_cwnd为单位增长。将循环改为除法。 tp-&gt;snd_cwnd_cnt += acked; if (tp-&gt;snd_cwnd_cnt &gt;= w) &#123; u32 delta = tp-&gt;snd_cwnd_cnt / w; tp-&gt;snd_cwnd_cnt -= delta * w; tp-&gt;snd_cwnd += delta; &#125; tp-&gt;snd_cwnd = min(tp-&gt;snd_cwnd, tp-&gt;snd_cwnd_clamp);&#125; @小刘1悦","tags":[{"name":"TCP","slug":"TCP","permalink":"http://yoursite.com/tags/TCP/"},{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"拥塞控制","slug":"拥塞控制","permalink":"http://yoursite.com/tags/拥塞控制/"}]},{"title":"TCP简介-读书笔记","date":"2017-03-19T12:00:38.000Z","path":"2017/03/19/TCP简介-读书笔记/","text":"@(Network)[tcp, congestion control] 读书笔记-TCP简介本文主要记录阅读linuxtcp文章，其第二章中主要介绍了TCP拥塞控制的基础和一些发展历程，这里作为整理。 个人理解，TCP的拥塞分为两部分，一部分是窗口值变化，慢启动和各种拥塞避免算法，这部分只尝试控制发往网络中的包的数量(拥塞窗口)，但是他并不处理是否丢包，是否应该重传，或者快速重传等；另一部分是TCP逐渐支持的一些机制，可以使tcp更好的进行网络状态的估计，不同的网络状态会对第一部分进行反馈，这部分属于框架级别(其实丢包重传和拥塞控制本身并没有完全相关性(比如固定丢包的链路，无线网络等)，但是由于长久以来的拥塞控制是基于丢包的，因此丢包作为网络拥塞状态判断，耦合在TCP的拥塞控制中，而由于丢包对拥塞控制的影响是毁灭性的，因此也在重传上做了较多的处理，来避免因为一些意外的丢包造成的影响，或者是提早避免因为网络拥塞导致的丢包)因此对应的tcp内核中，一部分对应的拥塞控制状态机，这里处理了所有tcp所支持的机制，如SACK,DSACK,FRTO等网络估计的基础，而拥塞避免则会根据猜测的网络状态进行合理控制。 [TOC] 0. 参考资料[1] linux_tcp[2] SACK[3] Duplicate SACK[4] Duplicate SACK ppt[5] FACK[6] FRTO-4138[7] FRTO-5862[8] FRTO-细节[9] An Enhanced Recovery Algorithm for TCP Retransmission Timeouts 1. 慢启动和拥塞避免TCP拥塞控制算法主要由发送端通过拥塞窗口(cwnd)来控制。最开始主要有两个拥塞控制的方法，通过阈值ssthresh来作为两种窗口增长的临界标志。 慢启动：在小于阈值时，当每一个ack到来时，慢启动算法会将拥塞窗口增加一个segment大小。当你有cwnd个包发出，收到cwnd个ack，在慢启动阶段将会增加cwnd个窗口大小，是指数增长的过程。 拥塞避免：当大于阈值后，拥塞避免算法会限制拥塞窗口在一个RTT内只增加一个segment大小。当你有cwnd个包发出，在这个RTT内收到cwnd个ack，在拥塞避免阶段每个ack增加(1 / 拥塞窗口大小)，窗口增加 (1 / cwnd) * cwnd = 1个大小。 2. RTO重传可能被重传定时器RTO触发，RTO一旦超时，表明某个包丢失，对于TCP而言，丢包意味着网络产生拥塞，因此此时会把拥塞窗口降低到最小值(1个segment大小)。因此丢包是非常严苛的拥塞条件，一旦丢包发生，对整个传输效率会造成极大的影响。 In addition, when RTO occurs, the sender resets the congestion window to one segment, since the RTO may indicate that the network load has changed dramatically. This is done because the packet loss is taken as an indication of congestion, and the sender needs to reduce its transmission rate to alleviate the network congestion. 3. RTT与重传二义性由于RTO本身对于TCP性能而言非常严苛，因此RTO使用的定时器时间是否能准确反映网络实际传输情况对于TCP而言非常重要。比如链路RTT突然增加了，但是用于RTO的RTT仍然是之前的小值，这样可能会导致数据虽然没有丢包，但是交往较慢，RTO触发过于频繁，再由于丢包对于TCP的影响，会导致TCP窗口衰减剧烈。 但是TCP对于普通包和重传包使用相同的sequence number，因此当ack到来时，无法区分这个ack对应的是普通包的还是重传包的，因此此时用这个ack的时间戳选项来计算链路RTT可能会导致各种问题。 因此当前使用的RTT计算公式一般是smooth rtt计算。 4. 快速重传和快速恢复由于丢包属于一类重大事故，因此TCP中总是需要尝试提前发现，在其形成重大事故(重置cwnd)前将其提前识别。因此当接收端收到乱序包时，会发送期待的包的序号ack，当发送端收到两个重复的ack时，会发现出现乱序(状态机中的Disorder)，并尝试进行重传来恢复这个包，以免RTO超时触发。当发送端收到三个重复的ack时，会进入快速恢复(状态机中的Recovery)，认为网络可能存在一定的拥塞，会降低拥塞窗口(但是不会像RTO触发以后那样激进)。两种状态下，都会恢复认为丢失或者乱序的包，直到收到非重复的ack为止。 5. 显式拥塞控制由于TCP本身并无法感知到整个网络链路的质量，因此基本是基于自己的算法和丢包反馈来进行拥塞判断，本身存在一定的局限性。 后来的一些路由器在处理数据包，可能可以感知到网络是否真正拥塞，当路由器感知到拥塞时，会通过设置TCP标志位的ECE给TCP，TCP发送端收到带有ECE的ack时会进入拥塞状态(状态机中的CWR)，同时发送一个携带标志位CWR的包给接收端，表示自己当前正在衰减拥塞窗口。 6. Selective acknowledgements由于进入乱序或者快速重传后，在一个RTT之内只能处理一个异常包(不会发送新的包直到新的ack到来，但是当丢包是不连续的若干个，恢复完后仍然会进入对应状态，单独处理下一个乱序丢包，如发送端发送1-6，接收端先收到1，3，6，这时候发送先恢复包2，恢复完包2收到ack发现乱序，请求包4，发送端再恢复4，恢复后发现仍然乱序，请求包5，再次单独恢复包5)，因此在恢复阶段严重影响吞吐量。 SACK并没有打破原有的ack机制，只是在其ack机制上，在TCP option字段中附加了额外信息。4 例子如下图： Multiple packet losses from a window of data can have a catastrophic effect on TCP throughput. SACK does not change the meaning of ACK field. 注: SACK附加在TCP option字段中，option字段最多只有40字节，因此SACK最多包含四个区间。 A SACK option that specifies n blocks will have a length of 8*n+2 bytes, so the 40 bytes available for TCP options can specify a maximum of 4 blocks. 注2: SACK必须接收和发送端都支持才可以正常使用。 7. Duplicate-SACKSACK在rfc 2018定义的时候，并没有声明其收到两个相同的包以后的处理。D-SACK会使接收端发送重复块的信息给发送端。 简单流程如下：当DSACK激活时，最后一个ACK中的SACK第一个区域为重复区域，不同于普通SACK，它是已经收到的区域的一个子区间，每个重复块只会上报一次。 8. Forward AcknowledgementsForward Acknowledgement是基于SACK的相关的拥塞控制算法。 当拥塞发生时，此时已经发生丢包，这时候会引入新的变量fackets_out来统计SACK中的数据量，并根据当前已经发送的数据量una和重传数据量retran来估算本条连接实际在网络中的包数量(总包数 - 已经ACK的数量 + 重传包)。并根据这个数值和拥塞窗口进行比较，如果当前网络中的包数量小于拥塞窗口，说明仍然可以往网络中发送部分数量的包。 这种算法本质上是对网络中的包数量进行更精确的估计，结合DSACK，可以更精准的进行判断，可以在恢复阶段依旧保持一定的速率，在处理乱序包的时候可以比传统TCP更加激进。 9. FRTO当网络链路存在一些突发的特殊场景时，可能会触发超时定时器，由于TCP对于丢包的处理异常严格，可能会造成链路质量下降。 可能的一些场景： 对于移动信号的跨域处理，可能会造成突发的延迟。 对于低带宽链路，偶发的竞争可能也会造成整个RTT的增加。 稳定的链路上可能也有一些原因导致某些包及其重传老是失败。 First, some mobile networking technologies involve sudden delay spikes on transmission because of actions taken during a hand-off. Second, given a low-bandwidth link or some other change in available bandwidth, arrival of competing traffic (possibly with higher priority) can cause a sudden increase of round-trip time. This may trigger a spurious retransmission timeout. A persistently reliable link layer can also cause a sudden delay when a data frame and several retransmissions of it are lost for some reason. 可能造成的一些影响： 虚假超时后的慢启动可能会导致向已经产生拥塞的网络中注入更多的数据包，会严重影响实际网络的拥塞状态。 当虚假超时触发后，可能造成虚假重传，当过多虚假重传发生后，对应的ack回来时可能会触发虚假的快速恢复，如下图，第二次虚假的快速重传是由于第一次虚假RTO超时导致重传发送了重复包导致。 However, if the RTO occurs spuriously and there still are segments outstanding in the network, a false slow start is harmful for the potentially congested network as it injects extra segments to the network at increasing rate. FRTO会在RTO超时后，不会类似传统TCP的超时机制，会额外根据后续两个ACK与当前未确认的最小包进行比较，根据这个结果判断当前RTO是否在安全范围内。如果收到的是未确认的包之后的包，则可能是因为网络原因导致的延迟，可以进入恢复状态。如果收到的是重复的ack，则认为这个包确实已经丢失，进入丢包状态。 10. 概览图简单用图画出之间衍生的关系。","tags":[{"name":"TCP","slug":"TCP","permalink":"http://yoursite.com/tags/TCP/"},{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"拥塞控制","slug":"拥塞控制","permalink":"http://yoursite.com/tags/拥塞控制/"},{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/tags/读书笔记/"}]},{"title":"九层妖塔","date":"2017-03-19T11:18:07.000Z","path":"2017/03/19/九层妖塔/","text":"3只精灵龙的其他故事涛哥分析的好有道理下一期涛哥应该要带来猜奥秘的模型了吧！ 以下代码待验证。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;class tree &#123; public: void build_tree (int dragon, int damage) &#123; vector&lt;int&gt; in; in.push_back (30); for (int i = 1; i &lt; dragon + 1; i++) &#123; in.push_back (2); &#125; build_tree_1 (in, damage, 1.0); &#125; bool result_match (vector&lt;int&gt; &amp;in) &#123; int cnt = 0; for (int i = 1; i &lt; in.size (); i++) if (in[i] != 0) cnt++; return cnt == alive; &#125; void build_tree_1 (vector&lt;int&gt; &amp;in, int damage, float p) &#123; int count = 0; float new_p = 0.0;#ifdef DEBUG for (int i = 0; i &lt; in.size (); i++) cout &lt;&lt; in[i] &lt;&lt; \",\"; cout &lt;&lt; \"Prob \" &lt;&lt; p &lt;&lt; \" else damage \" &lt;&lt; damage &lt;&lt; endl;#endif if (damage == 0) &#123; if (result_match (in)) result += p; return; &#125; for (int i = 0; i &lt; in.size (); i++) &#123; if (in[i] != 0) count++; &#125; if (count == 0) return; new_p = p / count; count = 0; for (int i = 0; i &lt; in.size (); i++) &#123; /* A dead dragon. */ if (in[i] == 0) continue; in[i]--; build_tree_1 (in, damage - 1, new_p); in[i]++; &#125; &#125; tree (int dragon, int damage, int _alive) : alive (_alive) &#123; result = 0.0; build_tree (dragon, damage); &#125; float get_result () &#123; return result; &#125; ~tree () &#123; &#125; private: int alive; float result;&#125;;int main ()&#123; tree t (3, 8, 1); cout &lt;&lt; t.get_result () &lt;&lt; endl; return 0;&#125;","tags":[{"name":"涛哥说的都怼","slug":"涛哥说的都怼","permalink":"http://yoursite.com/tags/涛哥说的都怼/"},{"name":"fun","slug":"fun","permalink":"http://yoursite.com/tags/fun/"}]}]