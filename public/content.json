[{"title":"Webrtc delay-base-bwe代码分析(6): 整体分析","date":"2017-05-22T11:44:23.000Z","path":"2017/05/22/Webrtc-delay-base-bwe代码分析-6-整体分析/","text":"当收到RTP数据包时，会触发RemoteBitrateEstimatorSingleStream::IncomingPacket函数进行处理。这里面使用到了之前几篇文章分析的模块，各自进行各自的处理。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374void RemoteBitrateEstimatorSingleStream::IncomingPacket( int64_t arrival_time_ms, size_t payload_size, const RTPHeader&amp; header) &#123; uint32_t ssrc = header.ssrc; uint32_t rtp_timestamp = header.timestamp + header.extension.transmissionTimeOffset; int64_t now_ms = clock_-&gt;TimeInMilliseconds(); CriticalSectionScoped cs(crit_sect_.get()); SsrcOveruseEstimatorMap::iterator it = overuse_detectors_.find(ssrc); // 根据SSRC查找对应的检测器 if (it == overuse_detectors_.end()) &#123; // This is a new SSRC. Adding to map. // TODO(holmer): If the channel changes SSRC the old SSRC will still be // around in this map until the channel is deleted. This is OK since the // callback will no longer be called for the old SSRC. This will be // automatically cleaned up when we have one RemoteBitrateEstimator per REMB // group. std::pair&lt;SsrcOveruseEstimatorMap::iterator, bool&gt; insert_result = overuse_detectors_.insert(std::make_pair( ssrc, new Detector(now_ms, OverUseDetectorOptions(), true))); it = insert_result.first; &#125; Detector* estimator = it-&gt;second; estimator-&gt;last_packet_time_ms = now_ms; // Check if incoming bitrate estimate is valid, and if it needs to be reset. // 会先以当前时间为基准，从历史的传输数据中尝试获取未更新当前时间的旧码率值。 rtc::Optional&lt;uint32_t&gt; incoming_bitrate = incoming_bitrate_.Rate(now_ms); if (incoming_bitrate) &#123; last_valid_incoming_bitrate_ = *incoming_bitrate; &#125; else if (last_valid_incoming_bitrate_ &gt; 0) &#123; // Incoming bitrate had a previous valid value, but now not enough data // point are left within the current window. Reset incoming bitrate // estimator so that the window size will only contain new data points. incoming_bitrate_.Reset(); last_valid_incoming_bitrate_ = 0; &#125; // 在历史传输数据中更新当前数据。 incoming_bitrate_.Update(payload_size, now_ms); const BandwidthUsage prior_state = estimator-&gt;detector.State(); uint32_t timestamp_delta = 0; int64_t time_delta = 0; int size_delta = 0; // 到达时间统计已经有足够样本构成一个新的group时， if (estimator-&gt;inter_arrival.ComputeDeltas( rtp_timestamp, arrival_time_ms, now_ms, payload_size, &amp;timestamp_delta, &amp;time_delta, &amp;size_delta)) &#123; double timestamp_delta_ms = timestamp_delta * kTimestampToMs; // 根据到达时间间隔，更新卡尔曼滤波器 estimator-&gt;estimator.Update(time_delta, timestamp_delta_ms, size_delta, estimator-&gt;detector.State()); // 根据卡尔曼滤波器校准后的值，更新当前链路使用状态。 estimator-&gt;detector.Detect(estimator-&gt;estimator.offset(), timestamp_delta_ms, estimator-&gt;estimator.num_of_deltas(), now_ms); &#125; // 当处于连续过载状态时候需要立刻对码率进行更新， // 如果是其他状态，会由定时器在Process函数中更新对应状态。 // UpdateEstimate会对状态机AimdRateControl进行处理。 if (estimator-&gt;detector.State() == kBwOverusing) &#123; rtc::Optional&lt;uint32_t&gt; incoming_bitrate_bps = incoming_bitrate_.Rate(now_ms); if (incoming_bitrate_bps &amp;&amp; (prior_state != kBwOverusing || remote_rate_-&gt;TimeToReduceFurther(now_ms, *incoming_bitrate_bps))) &#123; // The first overuse should immediately trigger a new estimate. // We also have to update the estimate immediately if we are overusing // and the target bitrate is too high compared to what we are receiving. UpdateEstimate(now_ms); &#125; &#125;&#125; 简单流程图：","tags":[{"name":"拥塞控制","slug":"拥塞控制","permalink":"http://yoursite.com/tags/拥塞控制/"},{"name":"webrtc","slug":"webrtc","permalink":"http://yoursite.com/tags/webrtc/"}]},{"title":"Webrtc delay-base-bwe代码分析(5): AimdRateControl模块","date":"2017-05-22T11:38:38.000Z","path":"2017/05/22/Webrtc-delay-base-bwe代码分析-5-AimdRateControl模块/","text":"0. 简介这个模块是根据OveruseDetector模块计算出来的状态来维护码率控制模块的自动状态机，并更新估算出来的对端发送速率，提供给REMB进行反馈。 1. 原理 一共维持三个状态，增长、保持、衰减，状态转换根据OveruseDetector的三个状态(Normal, Overuse, Underuse)来进行判断。 当Overuse发生时，无论什么状态都进入衰减。 当Underuse发生时，无论什么状态都进入保持状态。 在保持和增长阶段，Normal状态将保持继续增长。 在衰减阶段，Normal状态会将状态拉回保持状态。 2. 代码核心函数为ChangeBitrate，其他部分代码比较简单这里不贴了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119uint32_t AimdRateControl::ChangeBitrate(uint32_t current_bitrate_bps, uint32_t incoming_bitrate_bps, int64_t now_ms) &#123; // 在调用函数update更新对应的链路状态估计，累积码率，噪声值后 // 会将updated置位，如果没置位则不会去更新码率。 if (!updated_) &#123; return current_bitrate_bps_; &#125; // An over-use should always trigger us to reduce the bitrate, even though // we have not yet established our first estimate. By acting on the over-use, // we will end up with a valid estimate. // 初始化未完成，如果不是一开始就Overuse，直接返回初始的码率即可。 if (!bitrate_is_initialized_ &amp;&amp; current_input_.bw_state != kBwOverusing) return current_bitrate_bps_; updated_ = false; // 这里对状态进行转换，这个函数是状态机状态转换函数 // 1. Underuse总是进入Hold状态。 // 2. Overuse总是进入Dec状态。 // 3. Normal状态维持，除非当前在Hold状态，此时会进入Inc状态。 ChangeState(current_input_, now_ms); // Calculated here because it's used in multiple places. const float incoming_bitrate_kbps = incoming_bitrate_bps / 1000.0f; // Calculate the max bit rate std dev given the normalized // variance and the current incoming bit rate. const float std_max_bit_rate = sqrt(var_max_bitrate_kbps_ * avg_max_bitrate_kbps_); switch (rate_control_state_) &#123; // 保持状态不更新码率 case kRcHold: break; case kRcIncrease: // 三个状态，在最大值附近，超过最大值，比最大值高到不知道哪里去 // 最大均值已初始化，且当前码率高于最大值加上三倍方差，此时进入 // 比最大值高到不知道哪里去的状态，同时认为这个均值并不是很好使，复位。 // Above声明了，但是没有找到相应调用点。 if (avg_max_bitrate_kbps_ &gt;= 0 &amp;&amp; incoming_bitrate_kbps &gt; avg_max_bitrate_kbps_ + 3 * std_max_bit_rate) &#123; ChangeRegion(kRcMaxUnknown); avg_max_bitrate_kbps_ = -1.0; &#125; // if (rate_control_region_ == kRcNearMax) &#123; // Approximate the over-use estimator delay to 100 ms. // 已经接近最大值了，此时增长需谨慎，加性增加。 const int64_t response_time = rtt_ + 100; uint32_t additive_increase_bps = AdditiveRateIncrease( now_ms, time_last_bitrate_change_, response_time); current_bitrate_bps += additive_increase_bps; &#125; else &#123; // 由于没有Above状态的使用，因此认为比最大值高到不知道哪里去的状态属于 // 上界未定，放开手倍增码率。 uint32_t multiplicative_increase_bps = MultiplicativeRateIncrease( now_ms, time_last_bitrate_change_, current_bitrate_bps); current_bitrate_bps += multiplicative_increase_bps; &#125; time_last_bitrate_change_ = now_ms; break; case kRcDecrease: bitrate_is_initialized_ = true; if (incoming_bitrate_bps &lt; min_configured_bitrate_bps_) &#123; // 真的不能再低了.... current_bitrate_bps = min_configured_bitrate_bps_; &#125; else &#123; // Set bit rate to something slightly lower than max // to get rid of any self-induced delay. current_bitrate_bps = static_cast&lt;uint32_t&gt;(beta_ * incoming_bitrate_bps + 0.5); if (current_bitrate_bps &gt; current_bitrate_bps_) &#123; // 本次速率仍然在增长 // Avoid increasing the rate when over-using. if (rate_control_region_ != kRcMaxUnknown) &#123; // 如果上界可靠，则将码率设置在最大均值的beta_倍处， // 默认的beta_为0.85，同paper。 current_bitrate_bps = static_cast&lt;uint32_t&gt;( beta_ * avg_max_bitrate_kbps_ * 1000 + 0.5f); &#125; // 进行修正，和上一轮迭代的码率取小，如果上界不定 // 则取上一次迭代的码率值。 current_bitrate_bps = std::min(current_bitrate_bps, current_bitrate_bps_); &#125; // 更新过新的码率值后，认为现在已经在最大均值附近。 // 注意，每次认为上界无效时，总会把最大均值复位 // 这里设置完对应状态后，即使上界无效，下面总会更新一个最大均值。 ChangeRegion(kRcNearMax); if (incoming_bitrate_kbps &lt; avg_max_bitrate_kbps_ - 3 * std_max_bit_rate) &#123; // 当前速率小于均值较多，认为均值不可靠，复位 avg_max_bitrate_kbps_ = -1.0f; &#125; // 衰减状态下需要更新最大均值 UpdateMaxBitRateEstimate(incoming_bitrate_kbps); &#125; // Stay on hold until the pipes are cleared. // 降低码率后回到HOLD状态，如果网络状态仍然不好，在Overuse仍然会进入Dec状态。 // 如果恢复，则不会是Overuse，会保持或增长。 ChangeState(kRcHold); time_last_bitrate_change_ = now_ms; break; default: assert(false); &#125; if ((incoming_bitrate_bps &gt; 100000 || current_bitrate_bps &gt; 150000) &amp;&amp; current_bitrate_bps &gt; 1.5 * incoming_bitrate_bps) &#123; // Allow changing the bit rate if we are operating at very low rates // Don't change the bit rate if the send side is too far off current_bitrate_bps = current_bitrate_bps_; time_last_bitrate_change_ = now_ms; &#125; return current_bitrate_bps;&#125; 加性码率增长代码如下： 123456789101112131415161718192021uint32_t AimdRateControl::AdditiveRateIncrease( int64_t now_ms, int64_t last_ms, int64_t response_time_ms) const &#123; assert(response_time_ms &gt; 0); double beta = 0.0; if (last_ms &gt; 0) &#123; // 时间间隔和RTT之比作为系数。 // 疑问，这里的时间点是经过采样的，可能会大于rtt？ beta = std::min((now_ms - last_ms) / static_cast&lt;double&gt;(response_time_ms), 1.0); if (in_experiment_) beta /= 2.0; &#125; // 默认30fps，由于每个包不超过mtu，一般也就1100+，用这两个值估计每帧码率和每帧包数。 // 并计算平均每个包的大小，最终增加的比特数不超过1000。 double bits_per_frame = static_cast&lt;double&gt;(current_bitrate_bps_) / 30.0; double packets_per_frame = std::ceil(bits_per_frame / (8.0 * 1200.0)); double avg_packet_size_bits = bits_per_frame / packets_per_frame; uint32_t additive_increase_bps = std::max( 1000.0, beta * avg_packet_size_bits); return additive_increase_bps;&#125; 乘性部分比较简单，也是根据时间差来调整系数。 12345678910111213uint32_t AimdRateControl::MultiplicativeRateIncrease( int64_t now_ms, int64_t last_ms, uint32_t current_bitrate_bps) const &#123; double alpha = 1.08; if (last_ms &gt; -1) &#123; // 系数计算与文档中的1.05略有不同，使用时间差作为系数，1.08作为底数。 int time_since_last_update_ms = std::min(static_cast&lt;int&gt;(now_ms - last_ms), 1000); alpha = pow(alpha, time_since_last_update_ms / 1000.0); &#125; uint32_t multiplicative_increase_bps = std::max( current_bitrate_bps * (alpha - 1.0), 1000.0); return multiplicative_increase_bps;&#125; 最后一个是最大均值和方差的更新，主要在衰减状态时候进行估计。 12345678910111213141516171819202122232425void AimdRateControl::UpdateMaxBitRateEstimate(float incoming_bitrate_kbps) &#123; const float alpha = 0.05f; // 当前没有初始值，先设为当前码率，如果有的话，就用当前的值和均值做平滑。 if (avg_max_bitrate_kbps_ == -1.0f) &#123; avg_max_bitrate_kbps_ = incoming_bitrate_kbps; &#125; else &#123; avg_max_bitrate_kbps_ = (1 - alpha) * avg_max_bitrate_kbps_ + alpha * incoming_bitrate_kbps; &#125; // Estimate the max bit rate variance and normalize the variance // with the average max bit rate. const float norm = std::max(avg_max_bitrate_kbps_, 1.0f); // 方差的平滑 var_max_bitrate_kbps_ = (1 - alpha) * var_max_bitrate_kbps_ + alpha * (avg_max_bitrate_kbps_ - incoming_bitrate_kbps) * (avg_max_bitrate_kbps_ - incoming_bitrate_kbps) / norm; // 0.4 ~= 14 kbit/s at 500 kbit/s if (var_max_bitrate_kbps_ &lt; 0.4f) &#123; var_max_bitrate_kbps_ = 0.4f; &#125; // 2.5f ~= 35 kbit/s at 500 kbit/s if (var_max_bitrate_kbps_ &gt; 2.5f) &#123; var_max_bitrate_kbps_ = 2.5f; &#125;&#125;","tags":[{"name":"拥塞控制","slug":"拥塞控制","permalink":"http://yoursite.com/tags/拥塞控制/"},{"name":"webrtc","slug":"webrtc","permalink":"http://yoursite.com/tags/webrtc/"}]},{"title":"Webrtc delay-base-bwe代码分析(4): OveruseDetector模块","date":"2017-05-22T11:36:07.000Z","path":"2017/05/22/Webrtc-delay-base-bwe代码分析-4-OveruseDetector模块/","text":"0. 简介这个模块主要是根据OveruseEstimator模块校正后的到达时间差来对链路使用状态进行评估，为有限自动状态机提供状态转换的条件，同时本模块还有GCC文档中提到的自适应阈值计算。 阈值自适应原因如下：个人理解： 防止某些网络状态比较极端，使链路评估总是处于比较极端的情况。 由于基于RTT算法的公平性问题，在对抗基于丢包的拥塞控制算法衰减快，容易造成自身饿死，例如除了GCC外还有一条TCP流。 1We show that the threshold (ti), used by the over-use detector, must be made adaptive otherwise two issues can occur: 1) the delay-based control action may have no effect when the size of the bottleneck queue along the path is not sufficiently large and 2) the GCC flow may be starved by a concurrent loss-based TCP flow. 1. 原理 公式如上，delta_T为到达时间差，k为增益系数，m(ti)为ti时刻的延迟。增益系数是自适应的，当本次延迟值在上一轮迭代的阈值范围内时，增益系数k会进行衰减，其他时候增益系数k将会增加。参数选择是一个可以调优的过程，这里暂不讨论，webrtc中也开辟了对应的实验性接口可供配置。 2. 代码代码实现比较简单明了，基本就是按照上述公式进行实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293BandwidthUsage OveruseDetector::Detect(double offset, double ts_delta, int num_of_deltas, int64_t now_ms) &#123; if (num_of_deltas &lt; 2) &#123; return kBwNormal; &#125; const double prev_offset = prev_offset_; prev_offset_ = offset; // 更新delta T. const double T = std::min(num_of_deltas, 60) * offset; BWE_TEST_LOGGING_PLOT(1, \"offset\", now_ms, T); BWE_TEST_LOGGING_PLOT(1, \"threshold\", now_ms, threshold_); if (T &gt; threshold_) &#123; // 过载，更新对应的过载时间，过载次数统计。 if (time_over_using_ == -1) &#123; // Initialize the timer. Assume that we've been // over-using half of the time since the previous // sample. time_over_using_ = ts_delta / 2; &#125; else &#123; // Increment timer time_over_using_ += ts_delta; &#125; overuse_counter_++; // 当且仅当总过载时间超过过载时间阈值，且其曲线仍成上升趋势(offset &gt;= prev_offset)， // 进入过载状态 // 其他场景维持原状态。 // 判断具有一定容忍，防止因某些爆发式场景导致异常的过载判断影响网络吞吐量。 // 过载状态解除只有T降低至阈值之下。 if (time_over_using_ &gt; overusing_time_threshold_ &amp;&amp; overuse_counter_ &gt; 1) &#123; if (offset &gt;= prev_offset) &#123; time_over_using_ = 0; overuse_counter_ = 0; hypothesis_ = kBwOverusing; &#125; &#125; &#125; // 清除过载相关参数 else if (T &lt; -threshold_) &#123; // 低负载 time_over_using_ = -1; overuse_counter_ = 0; hypothesis_ = kBwUnderusing; &#125; else &#123; // 正常状态 time_over_using_ = -1; overuse_counter_ = 0; hypothesis_ = kBwNormal; &#125; // 当过载发生时，T总是大于阈值，因此此时阈值增大。 // 当未发生过载时，T总是小于阈值，此时阈值缩小。 UpdateThreshold(T, now_ms); return hypothesis_;&#125;void OveruseDetector::UpdateThreshold(double modified_offset, int64_t now_ms) &#123; // 实验性质 if (!in_experiment_) return; // 初始化 if (last_update_ms_ == -1) last_update_ms_ = now_ms; // 修正值超过阈值可控制的最大范围，这里啥都不做，防止阈值过大。 if (fabs(modified_offset) &gt; threshold_ + kMaxAdaptOffsetMs) &#123; // Avoid adapting the threshold to big latency spikes, caused e.g., // by a sudden capacity drop. last_update_ms_ = now_ms; return; &#125; // 第二个公式，根据修正值的绝对值和当前阈值(即i - 1时刻)比较，计算增益系数 // 默认k_down_ = 0.00006, k_up_ = 0.004。 // 这里两个值虽然都是正的， // 但是增长和减小的符号在第一个式子中的fabs(modified_offset) - threshold_ const double k = fabs(modified_offset) &lt; threshold_ ? k_down_ : k_up_; const int64_t kMaxTimeDeltaMs = 100; int64_t time_delta_ms = std::min(now_ms - last_update_ms_, kMaxTimeDeltaMs); // 公式一，阈值自适应，根据新的增益系数对阈值进行修正。 threshold_ += k * (fabs(modified_offset) - threshold_) * time_delta_ms; // 阈值上下限内修正。 const double kMinThreshold = 6; const double kMaxThreshold = 600; threshold_ = std::min(std::max(threshold_, kMinThreshold), kMaxThreshold); last_update_ms_ = now_ms;&#125; 参考资料：[1] google congestion control","tags":[{"name":"拥塞控制","slug":"拥塞控制","permalink":"http://yoursite.com/tags/拥塞控制/"},{"name":"webrtc","slug":"webrtc","permalink":"http://yoursite.com/tags/webrtc/"}]},{"title":"Webrtc delay-base-bwe代码分析(3): OveruseEstimator模块","date":"2017-05-22T11:32:48.000Z","path":"2017/05/22/Webrtc-delay-base-bwe代码分析-3-OveruseEstimator模块/","text":"该模块是一个卡尔曼滤波，根据当前到达时间差和传输大小的值，对到达时间差进行滤波，计算更精准的到达时间差。 0. 卡尔曼滤波基础公式从参考文档中获得基础公式及对应变量意义。 公式： 变量： 1. OveruseEstimator的卡尔曼滤波公式 2. 代码分析2.1 update123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293void OveruseEstimator::Update(int64_t t_delta, double ts_delta, int size_delta, BandwidthUsage current_hypothesis) &#123; const double min_frame_period = UpdateMinFramePeriod(ts_delta); const double t_ts_delta = t_delta - ts_delta; double fs_delta = size_delta; ++num_of_deltas_; if (num_of_deltas_ &gt; kDeltaCounterMax) &#123; num_of_deltas_ = kDeltaCounterMax; &#125; // Update the Kalman filter. // 误差矩阵 // 加上协方差，为配置初始化值。 // process_noise_ Q // P(k) = AP(k - 1)AT + Q // 预测方程2 // 2.3 E_[0][0] += process_noise_[0]; E_[1][1] += process_noise_[1]; if ((current_hypothesis == kBwOverusing &amp;&amp; offset_ &lt; prev_offset_) || (current_hypothesis == kBwUnderusing &amp;&amp; offset_ &gt; prev_offset_)) &#123; E_[1][1] += 10 * process_noise_[1]; &#125; // h观测矩阵 // 2.2 const double h[2] = &#123;fs_delta, 1.0&#125;; // P * h转置 // 2.4 const double Eh[2] = &#123;E_[0][0]*h[0] + E_[0][1]*h[1], E_[1][0]*h[0] + E_[1][1]*h[1]&#125;; // t_ts_delta 为 zk，residual为 zk - H * xk // 2.7 const double residual = t_ts_delta - slope_*h[0] - offset_; const bool in_stable_state = (current_hypothesis == kBwNormal); const double max_residual = 3.0 * sqrt(var_noise_); // We try to filter out very late frames. For instance periodic key // frames doesn't fit the Gaussian model well. if (fabs(residual) &lt; max_residual) &#123; UpdateNoiseEstimate(residual, min_frame_period, in_stable_state); &#125; else &#123; UpdateNoiseEstimate(residual &lt; 0 ? -max_residual : max_residual, min_frame_period, in_stable_state); &#125; // denom = h * P * ht + R，var_noise为测量噪声协方差 // 2.5 const double denom = var_noise_ + h[0]*Eh[0] + h[1]*Eh[1]; // K卡尔曼增益矩阵 // K(k) = Eh / (HPkHT + R) // 校正方程3 // 2.6 const double K[2] = &#123;Eh[0] / denom, Eh[1] / denom&#125;; // Ikh I-Kh // 2.9 const double IKh[2][2] = &#123;&#123;1.0 - K[0]*h[0], -K[0]*h[1]&#125;, &#123;-K[1]*h[0], 1.0 - K[1]*h[1]&#125;&#125;; const double e00 = E_[0][0]; const double e01 = E_[0][1]; // Update state. // 使用IKh更新误差矩阵 // P(k) = (I - Kh)P(k - 1) // 校准方程5 // 2.10 E_[0][0] = e00 * IKh[0][0] + E_[1][0] * IKh[0][1]; E_[0][1] = e01 * IKh[0][0] + E_[1][1] * IKh[0][1]; E_[1][0] = e00 * IKh[1][0] + E_[1][0] * IKh[1][1]; E_[1][1] = e01 * IKh[1][0] + E_[1][1] * IKh[1][1]; // The covariance matrix must be positive semi-definite. bool positive_semi_definite = E_[0][0] + E_[1][1] &gt;= 0 &amp;&amp; E_[0][0] * E_[1][1] - E_[0][1] * E_[1][0] &gt;= 0 &amp;&amp; E_[0][0] &gt;= 0; assert(positive_semi_definite); if (!positive_semi_definite) &#123; LOG(LS_ERROR) &lt;&lt; \"The over-use estimator's covariance matrix is no longer \" \"semi-definite.\"; &#125; // 2.8 slope_ = slope_ + K[0] * residual; prev_offset_ = offset_; offset_ = offset_ + K[1] * residual;&#125; 2.2 噪声更新1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 这个函数主要提供下面函数需要用到的ts_delta，这里不是直接使用// 本次的ts_delta，而是根据历史时间窗口中最小时间值作为噪声估计中的ts_delta// 这个函数就是在一个固定的时间窗口中按时间顺序存放每一个ts_delta// 当数量超过时就从最早的时间开始pop// 然后遍历整个时间窗口选择最小的时间差值。double OveruseEstimator::UpdateMinFramePeriod(double ts_delta) &#123; double min_frame_period = ts_delta; if (ts_delta_hist_.size() &gt;= kMinFramePeriodHistoryLength) &#123; ts_delta_hist_.pop_front(); &#125; std::list&lt;double&gt;::iterator it = ts_delta_hist_.begin(); for (; it != ts_delta_hist_.end(); it++) &#123; min_frame_period = std::min(*it, min_frame_period); &#125; ts_delta_hist_.push_back(ts_delta); return min_frame_period;&#125;void OveruseEstimator::UpdateNoiseEstimate(double residual, double ts_delta, bool stable_state) &#123; // 仅在Normal状态下更新噪声值。 if (!stable_state) &#123; return; &#125; // Faster filter during startup to faster adapt to the jitter level // of the network. |alpha| is tuned for 30 frames per second, but is scaled // according to |ts_delta|. double alpha = 0.01; if (num_of_deltas_ &gt; 10*30) &#123; alpha = 0.002; &#125; // Only update the noise estimate if we're not over-using. |beta| is a // function of alpha and the time delta since the previous update. // 时间差越大，残差的比重越小 // 由于上面函数控制一个时间窗口，因此一个时间窗口内，其权重基本固定。 const double beta = pow(1 - alpha, ts_delta * 30.0 / 1000.0); // 更新均值 avg_noise_ = beta * avg_noise_ + (1 - beta) * residual; // 利用方差更新噪声值。 var_noise_ = beta * var_noise_ + (1 - beta) * (avg_noise_ - residual) * (avg_noise_ - residual); if (var_noise_ &lt; 1) &#123; var_noise_ = 1; &#125;&#125; 参考文档[1] 卡尔曼滤波简介","tags":[{"name":"拥塞控制","slug":"拥塞控制","permalink":"http://yoursite.com/tags/拥塞控制/"},{"name":"webrtc","slug":"webrtc","permalink":"http://yoursite.com/tags/webrtc/"}]},{"title":"Webrtc delay-base-bwe代码分析(2): InterArrival模块","date":"2017-05-22T11:28:31.000Z","path":"2017/05/22/Webrtc-delay-base-bwe代码分析-2-InterArrival模块/","text":"0. 参考文档[1] google congestion control[2] Rtp payload format for h264 1. 功能该模块主要对到达的时间进行小范围内的统计、采样，并根据一定的时间间隔计算出对应的延迟、传输大小变化。 123The arrival-time filter. The goal of this block is to produce an estimate m(ti) of the one way delay gradient. For this purpose, we employ a Kalman filter that estimatesm(ti) based on the measured one way delay gradient dm(ti) which is computed as follows: dm(t(i) = (t(i) - t(i-1))-(Ti - T(i-1))where Ti is the time at which the first packet of the i-th video frame has been sent and ti is the time at which the last packet that forms the video frame has been received. 2. 流程注：RTP的timestamp默认频率为90kHz。 以发送的时间戳为准，每5ms内发送的包划为一个时间戳范围内，组成一个TimestampGroup，同一个TimestampGroup内的包，后来的时间覆盖先到的。当某个包到来时，根据时间戳判断是一个新的TimestampGroup，此时才会执行延迟差值的计算，计算使用上一个group和当前group的时间值。 注2：这里代码不涉及使用绝对的时间，涉及sdp中的abs_time。注3：这里默认的一个group时间为5ms，对于rtp的timestamp默认频率，450个rtp timestamp为一个group的tick。 3. 代码流程比较简单，主要就是new group的判断，以及group时间的更新和差值计算。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120bool InterArrival::ComputeDeltas(uint32_t timestamp, int64_t arrival_time_ms, int64_t system_time_ms, size_t packet_size, uint32_t* timestamp_delta, int64_t* arrival_time_delta_ms, int* packet_size_delta) &#123; assert(timestamp_delta != NULL); assert(arrival_time_delta_ms != NULL); assert(packet_size_delta != NULL); bool calculated_deltas = false; if (current_timestamp_group_.IsFirstPacket()) &#123; // We don't have enough data to update the filter, so we store it until we // have two frames of data to process. // 尚未完全初始化 current_timestamp_group_.timestamp = timestamp; current_timestamp_group_.first_timestamp = timestamp; &#125; else if (!PacketInOrder(timestamp)) &#123; return false; &#125; else if (NewTimestampGroup(arrival_time_ms, timestamp)) &#123; // First packet of a later frame, the previous frame sample is ready. // 需要新开一个group，此时计算prev_group和current_group的差值。 if (prev_timestamp_group_.complete_time_ms &gt;= 0) &#123; // 发送时间的差值，即公式中的T(i) - T(i - 1) *timestamp_delta = current_timestamp_group_.timestamp - prev_timestamp_group_.timestamp; // 接收时间的差值，即公式中的t(i) - t(i - 1) *arrival_time_delta_ms = current_timestamp_group_.complete_time_ms - prev_timestamp_group_.complete_time_ms; // Check system time differences to see if we have an unproportional jump // in arrival time. In that case reset the inter-arrival computations. // 防止一些时间戳异常处理。 int64_t system_time_delta_ms = current_timestamp_group_.last_system_time_ms - prev_timestamp_group_.last_system_time_ms; if (*arrival_time_delta_ms - system_time_delta_ms &gt;= kArrivalTimeOffsetThresholdMs) &#123; LOG(LS_WARNING) &lt;&lt; \"The arrival time clock offset has changed (diff = \" &lt;&lt; *arrival_time_delta_ms - system_time_delta_ms &lt;&lt; \" ms), resetting.\"; Reset(); return false; &#125; // 乱序包的异常处理 if (*arrival_time_delta_ms &lt; 0) &#123; // The group of packets has been reordered since receiving its local // arrival timestamp. ++num_consecutive_reordered_packets_; // 连续乱序包的异常处理 if (num_consecutive_reordered_packets_ &gt;= kReorderedResetThreshold) &#123; LOG(LS_WARNING) &lt;&lt; \"Packets are being reordered on the path from the \" \"socket to the bandwidth estimator. Ignoring this \" \"packet for bandwidth estimation, resetting.\"; Reset(); &#125; return false; &#125; else &#123; num_consecutive_reordered_packets_ = 0; &#125; assert(*arrival_time_delta_ms &gt;= 0); *packet_size_delta = static_cast&lt;int&gt;(current_timestamp_group_.size) - static_cast&lt;int&gt;(prev_timestamp_group_.size); calculated_deltas = true; &#125; // 更新整组group prev_timestamp_group_ = current_timestamp_group_; // The new timestamp is now the current frame. current_timestamp_group_.first_timestamp = timestamp; current_timestamp_group_.timestamp = timestamp; current_timestamp_group_.size = 0; &#125; else &#123; // 还在之前的时间范围内，此时将rtp timestanp更新，更新为一个新值。 current_timestamp_group_.timestamp = LatestTimestamp( current_timestamp_group_.timestamp, timestamp); &#125; // Accumulate the frame size. // 更新group中的数据。 current_timestamp_group_.size += packet_size; current_timestamp_group_.complete_time_ms = arrival_time_ms; current_timestamp_group_.last_system_time_ms = system_time_ms; return calculated_deltas;&#125;// Assumes that |timestamp| is not reordered compared to// |current_timestamp_group_|.bool InterArrival::NewTimestampGroup(int64_t arrival_time_ms, uint32_t timestamp) const &#123; if (current_timestamp_group_.IsFirstPacket()) &#123; return false; &#125; else if (BelongsToBurst(arrival_time_ms, timestamp)) &#123; return false; &#125; else &#123; uint32_t timestamp_diff = timestamp - current_timestamp_group_.first_timestamp; // rtp时间差大于一个Tick，即 90 * 5 个rtp时间采样点。 return timestamp_diff &gt; kTimestampGroupLengthTicks; &#125;&#125;// 允许group超出定死的group时间范围。// 当这个函数返回false，会认为不是一个新的group。bool InterArrival::BelongsToBurst(int64_t arrival_time_ms, uint32_t timestamp) const &#123; if (!burst_grouping_) &#123; return false; &#125; assert(current_timestamp_group_.complete_time_ms &gt;= 0); int64_t arrival_time_delta_ms = arrival_time_ms - current_timestamp_group_.complete_time_ms; uint32_t timestamp_diff = timestamp - current_timestamp_group_.timestamp; // 将rtp的时间差值转换为ms，乘上采样频率 1 / 90. int64_t ts_delta_ms = timestamp_to_ms_coeff_ * timestamp_diff + 0.5; if (ts_delta_ms == 0) return true; int propagation_delta_ms = arrival_time_delta_ms - ts_delta_ms; // 传播时间差，即当前包的RTT小于上一个包的RTT，且到达时间差小于爆发的阈值 return propagation_delta_ms &lt; 0 &amp;&amp; arrival_time_delta_ms &lt;= kBurstDeltaThresholdMs;&#125;","tags":[{"name":"拥塞控制","slug":"拥塞控制","permalink":"http://yoursite.com/tags/拥塞控制/"},{"name":"webrtc","slug":"webrtc","permalink":"http://yoursite.com/tags/webrtc/"}]},{"title":"Webrtc delay-base-bwe代码分析(1): RateStatistics模块","date":"2017-05-22T11:18:59.000Z","path":"2017/05/22/Webrtc-delay-base-bwe代码分析-1-RateStatistics模块/","text":"RateStatistics这个类的作用为记录一个时间窗口内的速率值，并返回当前时间区域内的码率值。单独开一个文章主要是用来描述其用来记录速率值的桶，一开始看的比较迷糊。 123456789101112131415161718192021222324252627282930313233class RateStatistics &#123; // Counters are kept in buckets (circular buffer), with one bucket // per millisecond. // 每毫秒一个记录值。 struct Bucket &#123; size_t sum; // Sum of all samples in this bucket. size_t samples; // Number of samples in this bucket. &#125;; std::unique_ptr&lt;Bucket[]&gt; buckets_; // 统计总值，一个为总的传输字节数，一个为这个单位时间内，总的采样点数。 // Total count recorded in buckets. size_t accumulated_count_; // The total number of samples in the buckets. size_t num_samples_; // 某个采样时间内的时间起点。包括时间值和索引值。 // Oldest time recorded in buckets. int64_t oldest_time_; // Bucket index of oldest counter recorded in buckets. uint32_t oldest_index_; // 类的参数值 // To convert counts/ms to desired units const float scale_; // The window sizes, in ms, over which the rate is calculated. // 最大的时间窗口，单位为毫秒，每一毫秒都有一个独立的Bucket，形成一个桶，桶的索引以时间计算。 const int64_t max_window_size_ms_; int64_t current_window_size_ms_;&#125;; 数据结构主要分为三部分： 总的统计值，包括当前时间窗口内总的传输字节数和总的采样点个数。 当前时间窗口内的索引的初值，类似array[0]一样的存在，记录这个时间窗口内的第一个时间点及其对应的索引。 设置参数。 简单示例: 如上图中，假设原来基准点为t1, 窗口值为1000ms，此时的时间窗口范围为[t1, t1 + 999ms]，此时新来的包将时间窗口的范围更新了，更新为[t1 + 3ms, t1 + 1002ms]，此时需要从t1及对应的index开始遍历，将所有超过时间窗口下限的元素抛弃，此时新的时间窗口为[t1 + 3ms - 1000, t1 + 3ms]，每一个bucket对应的时间点值也发生变化了(物理含义)。而对于插入一个节点则容易的多，只需要将该点的时间减去当前时间下限，其差值即为Bucket对应的下标。 原理弄明白了，代码也比较简单。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778void RateStatistics::Update(size_t count, int64_t now_ms) &#123; if (now_ms &lt; oldest_time_) &#123; // Too old data is ignored. return; &#125; // 每次更新值之前，需要更新整个窗口的时间区间。 // 保证索引有效 EraseOld(now_ms); // First ever sample, reset window to start now. if (!IsInitialized()) oldest_time_ = now_ms; // now_ms在桶中的位置由基准时间(oldest_time)和自身时间的毫秒差作为索引。 uint32_t now_offset = static_cast&lt;uint32_t&gt;(now_ms - oldest_time_); RTC_DCHECK_LT(now_offset, max_window_size_ms_); uint32_t index = oldest_index_ + now_offset; if (index &gt;= max_window_size_ms_) index -= max_window_size_ms_; // 更新统计值 buckets_[index].sum += count; ++buckets_[index].samples; accumulated_count_ += count; ++num_samples_;&#125;rtc::Optional&lt;uint32_t&gt; RateStatistics::Rate(int64_t now_ms) const &#123; // Yeah, this const_cast ain't pretty, but the alternative is to declare most // of the members as mutable... const_cast&lt;RateStatistics*&gt;(this)-&gt;EraseOld(now_ms); // If window is a single bucket or there is only one sample in a data set that // has not grown to the full window size, treat this as rate unavailable. // 真正有效的时间区间。 int64_t active_window_size = now_ms - oldest_time_ + 1; // 采样点数不足时候返回无效值。 if (num_samples_ == 0 || active_window_size &lt;= 1 || (num_samples_ &lt;= 1 &amp;&amp; active_window_size &lt; current_window_size_ms_)) &#123; // 返回一个无效值 return rtc::Optional&lt;uint32_t&gt;(); &#125; // 码率的因子，最后的值为（总大小 * scale_ / 总的有效时间） float scale = scale_ / active_window_size; return rtc::Optional&lt;uint32_t&gt;( static_cast&lt;uint32_t&gt;(accumulated_count_ * scale + 0.5f));&#125;void RateStatistics::EraseOld(int64_t now_ms) &#123; if (!IsInitialized()) return; // New oldest time that is included in data set. // 根据窗口大小更新当前时间窗口，更新为[now_ms - 窗口， now_ms] int64_t new_oldest_time = now_ms - current_window_size_ms_ + 1; // New oldest time is older than the current one, no need to cull data. if (new_oldest_time &lt;= oldest_time_) return; // Loop over buckets and remove too old data points. // 从当前时间窗口下限开始逐毫秒遍历 while (num_samples_ &gt; 0 &amp;&amp; oldest_time_ &lt; new_oldest_time) &#123; const Bucket&amp; oldest_bucket = buckets_[oldest_index_]; RTC_DCHECK_GE(accumulated_count_, oldest_bucket.sum); RTC_DCHECK_GE(num_samples_, oldest_bucket.samples); // 从统计数据中将超时的Bucket中的数据扣除，并清空Bucket。 accumulated_count_ -= oldest_bucket.sum; num_samples_ -= oldest_bucket.samples; buckets_[oldest_index_] = Bucket(); // 环形 if (++oldest_index_ &gt;= max_window_size_ms_) oldest_index_ = 0; ++oldest_time_; &#125; oldest_time_ = new_oldest_time;&#125;","tags":[{"name":"拥塞控制","slug":"拥塞控制","permalink":"http://yoursite.com/tags/拥塞控制/"},{"name":"webrtc","slug":"webrtc","permalink":"http://yoursite.com/tags/webrtc/"}]},{"title":"webrtc中rtcp码率控制分析","date":"2017-05-09T03:22:45.000Z","path":"2017/05/09/webrtc中rtcp码率控制分析/","text":"0. 参考文档[1] google congestion control 1. 简介webrtc的带宽估计分为两部分，一部分为发送端根据rtcp反馈信息进行反馈，另一部分为接收端根据收到的rtp数据进行相应的码率估计[1]。本文先分析发送端根据rtcp反馈信息进行码率调整的部分代码。 具体计算公式: 2. 代码结构2.1 类关系 rtp_stream_receiver中有一个继承自抽象类RtpRtcp的ModuleRtpRtcpImpl，ModuleRtpRtcpImpl中有一个rtcp_receiver。当有RTCP包到来时，逐层处理至rtcp_receiver，当包是rtcp receiver report包，则会将包解析，然后在ModuleRtpRtcpImpl中再次调用rtcp_receiver中的TriggerCallbacksFromRTCPPacket函数，触发对应rtcp的一些事件，反馈触发的主要是_cbRtcpBandwidthObserver的观察者(RtcpBandwidthObserverImpl)，这个观察者收到对应的report block之后会计算成带宽估计所需要的参数，并调用属主bitratecontrolImpl类对带宽进行估计，这里会调用SendSideBandwidthEstimation中的UpdateReceiverBlock进行实际的带宽评估。 2.2 调用关系图 3. 代码分析3.1 HandleReportBlock这个函数中最主要的部分就是RTT的计算，webrtc中对于RTT平滑的因子是一个线性增长的因子。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120/* 这个函数根据对应的report block生成了一个新的RTCPReportBlockInformation结构体， * 并计算出对应的RTT，多report block在调用点处执行循环。 */void RTCPReceiver::HandleReportBlock( const RTCPUtility::RTCPPacket&amp; rtcpPacket, RTCPPacketInformation&amp; rtcpPacketInformation, uint32_t remoteSSRC) EXCLUSIVE_LOCKS_REQUIRED(_criticalSectionRTCPReceiver) &#123; // This will be called once per report block in the RTCP packet. // We filter out all report blocks that are not for us. // Each packet has max 31 RR blocks. // // We can calc RTT if we send a send report and get a report block back. // |rtcpPacket.ReportBlockItem.SSRC| is the SSRC identifier of the source to // which the information in this reception report block pertains. // Filter out all report blocks that are not for us. if (registered_ssrcs_.find(rtcpPacket.ReportBlockItem.SSRC) == registered_ssrcs_.end()) &#123; // This block is not for us ignore it. return; &#125; RTCPReportBlockInformation* reportBlock = CreateOrGetReportBlockInformation(remoteSSRC, rtcpPacket.ReportBlockItem.SSRC); if (reportBlock == NULL) &#123; LOG(LS_WARNING) &lt;&lt; \"Failed to CreateReportBlockInformation(\" &lt;&lt; remoteSSRC &lt;&lt; \")\"; return; &#125; // 用于RTCP超时的计算。 _lastReceivedRrMs = _clock-&gt;TimeInMilliseconds(); // 其他字段的拷贝。 const RTCPPacketReportBlockItem&amp; rb = rtcpPacket.ReportBlockItem; reportBlock-&gt;remoteReceiveBlock.remoteSSRC = remoteSSRC; reportBlock-&gt;remoteReceiveBlock.sourceSSRC = rb.SSRC; reportBlock-&gt;remoteReceiveBlock.fractionLost = rb.FractionLost; reportBlock-&gt;remoteReceiveBlock.cumulativeLost = rb.CumulativeNumOfPacketsLost; if (rb.ExtendedHighestSequenceNumber &gt; reportBlock-&gt;remoteReceiveBlock.extendedHighSeqNum) &#123; // We have successfully delivered new RTP packets to the remote side after // the last RR was sent from the remote side. _lastIncreasedSequenceNumberMs = _lastReceivedRrMs; &#125; reportBlock-&gt;remoteReceiveBlock.extendedHighSeqNum = rb.ExtendedHighestSequenceNumber; reportBlock-&gt;remoteReceiveBlock.jitter = rb.Jitter; reportBlock-&gt;remoteReceiveBlock.delaySinceLastSR = rb.DelayLastSR; reportBlock-&gt;remoteReceiveBlock.lastSR = rb.LastSR; if (rtcpPacket.ReportBlockItem.Jitter &gt; reportBlock-&gt;remoteMaxJitter) &#123; reportBlock-&gt;remoteMaxJitter = rtcpPacket.ReportBlockItem.Jitter; &#125; int64_t rtt = 0; uint32_t send_time = rtcpPacket.ReportBlockItem.LastSR; // RFC3550, section 6.4.1, LSR field discription states: // If no SR has been received yet, the field is set to zero. // Receiver rtp_rtcp module is not expected to calculate rtt using // Sender Reports even if it accidentally can. if (!receiver_only_ &amp;&amp; send_time != 0) &#123; // 当RR在SR之前发送，send_time为0. // delay计算: // Send SR Receive RR // | delay in RR | // | |&lt;-----------&gt;| | // |&lt;----------------------&gt;| |&lt;-----------------------&gt;| // // RTT = total_time - delay_in_RR // = receiver_rr_time - send_sr_time - delay_in_RR // 即使中间几个SR丢包，但是如果RTT本身是平滑的，那么RTT不会受到这几个丢包的影响 // 因为SR-&gt;RR之间的delay可以精确计算。 uint32_t delay = rtcpPacket.ReportBlockItem.DelayLastSR; // Local NTP time. uint32_t receive_time = CompactNtp(NtpTime(*_clock)); // RTT in 1/(2^16) seconds. uint32_t rtt_ntp = receive_time - delay - send_time; // Convert to 1/1000 seconds (milliseconds). rtt = CompactNtpRttToMs(rtt_ntp); if (rtt &gt; reportBlock-&gt;maxRTT) &#123; // Store max RTT. reportBlock-&gt;maxRTT = rtt; &#125; if (reportBlock-&gt;minRTT == 0) &#123; // First RTT. reportBlock-&gt;minRTT = rtt; &#125; else if (rtt &lt; reportBlock-&gt;minRTT) &#123; // Store min RTT. reportBlock-&gt;minRTT = rtt; &#125; // Store last RTT. reportBlock-&gt;RTT = rtt; // store average RTT // RTT的平滑计算。 // 如果这个块是在CreateOrGetReportBlockInformation新生成的， // 则权重会从0开始随着受到的report逐渐递增。 // srtt(i) = i/(i+1)*srtt(i-1) + 1/(i+1)*rtt + 0.5 if (reportBlock-&gt;numAverageCalcs != 0) &#123; float ac = static_cast&lt;float&gt;(reportBlock-&gt;numAverageCalcs); float newAverage = ((ac / (ac + 1)) * reportBlock-&gt;avgRTT) + ((1 / (ac + 1)) * rtt); reportBlock-&gt;avgRTT = static_cast&lt;int64_t&gt;(newAverage + 0.5f); &#125; else &#123; // First RTT. reportBlock-&gt;avgRTT = rtt; &#125; reportBlock-&gt;numAverageCalcs++; &#125; TRACE_COUNTER_ID1(TRACE_DISABLED_BY_DEFAULT(\"webrtc_rtp\"), \"RR_RTT\", rb.SSRC, rtt); // 添加回rtcpPacketInformation，在ModuleRtpRtcpImpl中会使用这个进行事件回调。 rtcpPacketInformation.AddReportInfo(*reportBlock);&#125; 3.2 UpdateMinHistory这个函数主要用于更新变量min_bitrate_history_，这个变量将会作用于上升区间，用来作为基数，这里简单描述下。 1234567891011121314151617181920212223242526272829303132// Updates history of min bitrates.// After this method returns min_bitrate_history_.front().second contains the// min bitrate used during last kBweIncreaseIntervalMs.// 主要结合这个函数解释下变量min_bitrate_history_// 这个变量的两个维度，front记录的是离当前最远的时间，// 每个速率都是按照时间先后顺序逐渐push到尾部。// 因此更新的时候，需要先将超时的元素从列表头剔除。// 后一个维度是最小速率值，// 在相同的时间区间内，保留最小的速率值。// |-------Interval 1---------|----------Interval 2------|// | | |// |--t1 &lt; t2 &lt; t3 &lt; t4 &lt; t5--|--t1 &lt; t2 &lt; t3 &lt; t4 &lt; t5--|// 这样的操作较为简单，不用在每次插入元素时去判断对应的时间区域，再找到对应时间区间的最小值，用部分冗余的内存换取操作的快捷。void SendSideBandwidthEstimation::UpdateMinHistory(int64_t now_ms) &#123; // Remove old data points from history. // Since history precision is in ms, add one so it is able to increase // bitrate if it is off by as little as 0.5ms. while (!min_bitrate_history_.empty() &amp;&amp; now_ms - min_bitrate_history_.front().first + 1 &gt; kBweIncreaseIntervalMs) &#123; min_bitrate_history_.pop_front(); &#125; // Typical minimum sliding-window algorithm: Pop values higher than current // bitrate before pushing it. while (!min_bitrate_history_.empty() &amp;&amp; bitrate_ &lt;= min_bitrate_history_.back().second) &#123; min_bitrate_history_.pop_back(); &#125; min_bitrate_history_.push_back(std::make_pair(now_ms, bitrate_));&#125; 3.3 UpdateEstimate函数UpdateReceiverBlock会根据当前的report block对当前带宽估计的一些变量进行相应的赋值，此外，只有当传输包的数量达到一定数量才会再次触发带宽估计的调整。函数UpdateEstimate是主要用于带宽估计的函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475void SendSideBandwidthEstimation::UpdateEstimate(int64_t now_ms) &#123; // We trust the REMB and/or delay-based estimate during the first 2 seconds if // we haven't had any packet loss reported, to allow startup bitrate probing. if (last_fraction_loss_ == 0 &amp;&amp; IsInStartPhase(now_ms)) &#123; uint32_t prev_bitrate = bitrate_; // bwe_incoming_是remb更新的值，如果当前无丢包且在启动阶段，直接使用remb的值。 if (bwe_incoming_ &gt; bitrate_) bitrate_ = CapBitrateToThresholds(now_ms, bwe_incoming_); ... &#125; &#125; UpdateMinHistory(now_ms); // Only start updating bitrate when receiving receiver blocks. // TODO(pbos): Handle the case when no receiver report is received for a very // long time. if (time_last_receiver_block_ms_ != -1) &#123; if (last_fraction_loss_ &lt;= 5) &#123; // Loss &lt; 2%: Increase rate by 8% of the min bitrate in the last // kBweIncreaseIntervalMs. // Note that by remembering the bitrate over the last second one can // rampup up one second faster than if only allowed to start ramping // at 8% per second rate now. E.g.: // If sending a constant 100kbps it can rampup immediatly to 108kbps // whenever a receiver report is received with lower packet loss. // If instead one would do: bitrate_ *= 1.08^(delta time), it would // take over one second since the lower packet loss to achieve 108kbps. //TODO:tjl // 这里与公式有一定不同： // 1. 系数不同，且附带一定的修正值(向上取整加1kbps) // 2. 取的是上一个时间间隔之内最小值，比较平滑。 bitrate_ = static_cast&lt;uint32_t&gt;( min_bitrate_history_.front().second * 1.08 + 0.5); // Add 1 kbps extra, just to make sure that we do not get stuck // (gives a little extra increase at low rates, negligible at higher // rates). bitrate_ += 1000; event_log_-&gt;LogBwePacketLossEvent( bitrate_, last_fraction_loss_, expected_packets_since_last_loss_update_); &#125; else if (last_fraction_loss_ &lt;= 26) &#123; // Loss between 2% - 10%: Do nothing. &#125; else &#123; // Loss &gt; 10%: Limit the rate decreases to once a kBweDecreaseIntervalMs + // rtt. if (!has_decreased_since_last_fraction_loss_ &amp;&amp; (now_ms - time_last_decrease_ms_) &gt;= (kBweDecreaseIntervalMs + last_round_trip_time_ms_)) &#123; time_last_decrease_ms_ = now_ms; // Reduce rate: // newRate = rate * (1 - 0.5*lossRate); // where packetLoss = 256*lossRate; //TODO:tjl // 当从未开始降低窗口值，且距离上一次衰减的时间差大于衰减周期加上rtt。 // 其实当前貌似只有这个case下会对这两个变量赋值。 // 这里的last_fraction_loss_是一次统计间隔(一定包数)之间的总丢包率。 // 丢包率的单位是1/256，因此这里是(1 - 丢包率/2) * 当前速率 // 与公式相同。 bitrate_ = static_cast&lt;uint32_t&gt;( (bitrate_ * static_cast&lt;double&gt;(512 - last_fraction_loss_)) / 512.0); has_decreased_since_last_fraction_loss_ = true; &#125; event_log_-&gt;LogBwePacketLossEvent( bitrate_, last_fraction_loss_, expected_packets_since_last_loss_update_); &#125; &#125; // 在有效范围内修正。 bitrate_ = CapBitrateToThresholds(now_ms, bitrate_);&#125;","tags":[{"name":"webrt","slug":"webrt","permalink":"http://yoursite.com/tags/webrt/"}]},{"title":"TCP的FRTO分析","date":"2017-04-11T11:04:12.000Z","path":"2017/04/11/TCP的FRTO分析/","text":"TCP的FRTO理解本文主要描述内核4.9.4中的TCP丢包处理与frto相关的操作，主要覆盖使用sack的场景。 0. 参考文档[1] RFC-5682[2] linuxtcp.ps[3] frto.pdf 1. FRTO要解决的问题FRTO主要是用来处理在DSACK生效时，突发的延迟触发RTO超时后，不必要的延迟和重传报文的ack造成了DSACK而产生非必要的快速重传[1]。传统的基于DSACK的RTO超时会有如下问题: 1) 在16.5s和17s中间最后一个ack到来之前一直处于慢启动阶段，指数发送数据， 2) 18s虚假RTO触发，其实只是延迟，但是这时候RTO工作，重传丢失的报文。 3) 19s的时候延迟包的ack到达，注意到此时ack的数据序列号是大于重传报文的序列号。 4) 延迟的ack使发送端继续重传之后延迟的ack之后的数据(这部分数据之前已经发送过))(19s到19.5s之间的重传)。 5) 在19.8s，之前发送的最大序列号报文被确认，接着由于4中的重传报文陆续到达，接收端发送了一系列以最大序列号报文为ack的Duplicate SACK。 6) 发送端收到这些重复的DSACK后，触发了快速重传，降低了传输性能。 使用FRTO后的效果: 1) 同上 2) 同上 3) 同上 4) 由于延迟包的ack更新了snd_una，因此这里不重传其余数据，而是发送两个新的分片。 5) 延迟包的ack陆续到达，此时由于未重传的包收到了对应的ack，因此可以判断当前是一个虚假的RTO，继续发送新的数据。 1. FRTO rfc解释这里主要针对sack的场景，即rfc-5682的section 3。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758591) When the retransmission timer expires, retransmit the first unacknowledged segment and set SpuriousRecovery to FALSE. Following the recommendation in the SACK specification [MMFR96], reset the SACK scoreboard. If &quot;RecoveryPoint&quot; is larger than or equal to SND.UNA, do not enter step 2 of this algorithm. Instead, set variable &quot;RecoveryPoint&quot; to indicate the highest sequence number transmitted so far and continue with slow-start retransmissions following the conventional RTO recovery algorithm.2) Wait until the acknowledgment of the data retransmitted due to the timeout arrives at the sender. If duplicate ACKs arrive before the cumulative acknowledgment for retransmitted data, adjust the scoreboard according to the incoming SACK information. Stay in step 2 and wait for the next new acknowledgment. If the retransmission timeout expires again, go to step 1 of the algorithm. When a new acknowledgment arrives, set variable &quot;RecoveryPoint&quot; to indicate the highest sequence number transmitted so far. a) If the Cumulative Acknowledgment field covers &quot;RecoveryPoint&quot; but not more than &quot;RecoveryPoint&quot;, revert to the conventional RTO recovery and set the congestion window to no more than 2 * MSS, like a regular TCP would do. Do not enter step 3 of this algorithm. b) Else, if the Cumulative Acknowledgment field does not cover &quot;RecoveryPoint&quot; but is larger than SND.UNA, transmit up to two new (previously unsent) segments and proceed to step 3. If the TCP sender is not able to transmit any previously unsent data -- either due to receiver window limitation or because it does not have any new data to send -- the recommended action is to refrain from entering step 3 of this algorithm. Rather, continue with slow-start retransmissions following the conventional RTO recovery algorithm. It is also possible to apply some of the alternatives for handling window-limited cases discussed in Appendix A.3) The next acknowledgment arrives at the sender. Either a duplicate ACK or a new cumulative ACK (advancing the window) applies in this step. Other types of ACKs are ignored without any action. a) If the Cumulative Acknowledgment field or the SACK information covers more than &quot;RecoveryPoint&quot;, set the congestion window to no more than 3 * MSS and proceed with the conventional RTO recovery, retransmitting unacknowledged segments. Take this branch also when the acknowledgment is a duplicate ACK and it does not acknowledge any new, previously unacknowledged data below &quot;RecoveryPoint&quot; in the SACK information. Leave SpuriousRecovery set to FALSE. b) If the Cumulative Acknowledgment field or a SACK information in the ACK does not cover more than &quot;RecoveryPoint&quot; AND it acknowledges data that was not acknowledged earlier (either with cumulative acknowledgment or using SACK information), declare the timeout spurious and set SpuriousRecovery to SPUR_TO. The retransmission timeout can be declared spurious, because the segment acknowledged with this ACK was transmitted before the timeout. 注1: RecoveryPoint为tcp中的high_seq，后称恢复点。注2: snd_nxt为下一个将要发送的包的包号。 3.1: 当重传定时器超时了，首先重传第一个未确认的分片，同时设置SpuriousRecovery为false，并重置sack的计分板。如果恢复点大于等于下一个要发的包(好像基本不可能，最多等于)，则将恢复点设置为当前发送的最大包。进入常规恢复。否则进入step 2。 3.2: 等待重传数据的ack到来。如果重复的ack比新的ack更早到达，则更新相应的sack计分板，同时留在第二步，等待新的ack到来。如果重传定时器再次超时，回到第一步。当新的ack到来后，更新恢复点。 a) 如果到来的ack中包括了恢复点，但不超过恢复点，撤销至普通恢复，同时拥塞窗口设置不大于2倍MSS。不进入第三步。 b) 如果到来的ack中不包含恢复点，但是收到的包超过未确认包，发送两个新的分片，并进入步骤3。如果当前发送端无法发送新报文(接收窗口限制或者没有新的应用层数据)，这里建议不要进入步骤3，而是使用恢复步骤。-3.3: 下一个ack到来，除了新的ack或者是重复ack，其他ack在这个步骤中都被忽略。 a) 如果新的ack包含了恢复点，则设置拥塞窗口不超过3倍MSS大小并执行恢复操作，重传未确认分片。这个分支也处理重复ack但是没有确认任何新块的场景。 b) 如果新的ack或者sack信息中没有包含恢复点，且确认的数据是之前没有确认的，则认为这个超时是一个奇怪的超时，并设置SpuriousRecovery为SPUR_TO。这个重传被标记为奇怪的，因为这个分片在超时前被确认了。 其实论文中的图比rfc这段(恢复点的几个判断理解费力。。)更好理解，也更契合代码，为了识别出虚假的RTO： 超时后先只重发丢失的一个包(3.1)。 判断重传后的第一个新的ack是否更新了snd_una，如果更新了snd_una，就发送新的数据，在判断虚假RTO之前不重传数据(3.2.b) 如果没重传就已经收到数据，尝试撤销本次RTO，如果撤销成功，这就是一个虚假RTO，进入恢复流程(3.3.b)。如果还是重复ack，则认为这是一个丢包事件(3.3.a)。 2. 4.9.4中的代码这里主要分析tcp_process_loss函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/* Process an ACK in CA_Loss state. Move to CA_Open if lost data are * recovered or spurious. Otherwise retransmits more on partial ACKs. */// 这个函数中如果可以恢复，则会直接进入CA_OPEN。// 不进入CA_OPEN则会直接返回tcp_ack中进行重传(或者是发送新分片)。static void tcp_process_loss(struct sock *sk, int flag, bool is_dupack, int *rexmit)&#123; struct tcp_sock *tp = tcp_sk(sk); // 完全恢复 bool recovered = !before(tp-&gt;snd_una, tp-&gt;high_seq); // 最大的未确认包已经发生改变，可能是之前丢包已经恢复，尝试撤销丢包状态。 // 当frto关闭时，这是唯二可以退出LOSS状态的条件。 if ((flag &amp; FLAG_SND_UNA_ADVANCED) &amp;&amp; tcp_try_undo_loss(sk, false)) return; // frto在enter_loss时候置位，根据超时时的设置决定是否可能是一个虚假的RTO。 if (tp-&gt;frto) &#123; /* F-RTO RFC5682 sec 3.1 (sack enhanced version). */ /* Step 3.b. A timeout is spurious if not all data are * lost, i.e., never-retransmitted data are (s)acked. */ // #define FLAG_ORIG_SACK_ACKED 0x200 /* Never retransmitted data are (s)acked */ // 包没有丢，因为还没重传就收到了ACK，只是延迟了，这是虚假RTO，撤销丢包状态。 // 注: 这里的frto_undo传参为true，必定恢复。 // 对应RTO论文图中的延迟包的acks。 if ((flag &amp; FLAG_ORIG_SACK_ACKED) &amp;&amp; tcp_try_undo_loss(sk, true)) return; if (after(tp-&gt;snd_nxt, tp-&gt;high_seq)) &#123; // 如果是刚进入LOSS状态，会先尝试重传，这时候snd_nxt总是等于high_seq的， // 这个分支主要对应3.2.b之后发送了两个新分片。 // 虽然发送了新分片，没有发重传，但是这时候收到的ack并没有更新una // 说明这个rtt中，之前una的包仍旧没有达到，因此这里认为他是真的超时 // 关闭frto。对应3.3.a if (flag &amp; FLAG_DATA_SACKED || is_dupack) tp-&gt;frto = 0; /* Step 3.a. loss was real */ &#125; else if (flag &amp; FLAG_SND_UNA_ADVANCED &amp;&amp; !recovered) &#123; // 这里进入的条件为 // 1. snd_nxt == high_seq，还没发送过新分片 // 2. una更新过，且没有完全恢复 // 执行3.2.b，发送新分片。 // 对应论文图中收到了一个更新过snd_una的ack。 tp-&gt;high_seq = tp-&gt;snd_nxt; /* Step 2.b. Try send new data (but deferred until cwnd * is updated in tcp_ack()). Otherwise fall back to * the conventional recovery. */ // 3.2.b中判断当前是否可以发送新分片。 if (tcp_send_head(sk) &amp;&amp; after(tcp_wnd_end(tp), tp-&gt;snd_nxt)) &#123; *rexmit = REXMIT_NEW; return; &#125; tp-&gt;frto = 0; &#125; &#125; // 已经完全恢复，则撤销对应的恢复操作，并进入TCP_CA_OPEN状态。后续将进入恢复状态。 // 这里主要处理了其他几个不在FRTO可处理的场景，如3.2.a和3.3.a // 唯一进入这里但frto还可能生效的场景为: // 发送新分片后，但是收到了一个不是新的sack，且不是一个dup sack。 // 在这种情况下的处理应该和上一个旧的sack相同。 // 个人理解应该是3.3中被忽略的case。 if (recovered) &#123; /* F-RTO RFC5682 sec 3.1 step 2.a and 1st part of step 3.a */ tcp_try_undo_recovery(sk); return; &#125; if (tcp_is_reno(tp)) &#123; /* A Reno DUPACK means new data in F-RTO step 2.b above are * delivered. Lower inflight to clock out (re)tranmissions. */ if (after(tp-&gt;snd_nxt, tp-&gt;high_seq) &amp;&amp; is_dupack) tcp_add_reno_sack(sk); else if (flag &amp; FLAG_SND_UNA_ADVANCED) tcp_reset_reno_sack(tp); &#125; *rexmit = REXMIT_LOST;&#125; 3. 简单流程图 3.1 常规frto流程 1) RTO超时，发送新分片 2) 收到一个ack，进入tcp_process_loss处理，此时frto开启，如果延迟的ack更新了una，则直接恢复loss状态。如果刚好等于重传包，这时候先发送新分片。 3) 又一个新的ack到来，这时候由于没有重传包，延迟的ack会更新una，直接撤销丢包处理，离开LOSS状态。 3.2 frto恢复失败流程 1) RTO超时，发送新分片 2) 收到一个ack，进入tcp_process_loss处理，此时frto开启，如上发送新分片。 3) 又一个新的ack到来，这时候由于丢包，不更新对应的una，因此关闭frto，进入重传流程。","tags":[{"name":"TCP","slug":"TCP","permalink":"http://yoursite.com/tags/TCP/"},{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"拥塞控制","slug":"拥塞控制","permalink":"http://yoursite.com/tags/拥塞控制/"}]},{"title":"小怼的世界","date":"2017-04-06T13:56:41.000Z","path":"2017/04/06/hello-world/","text":"先打(怼)个(下)招(涛)呼(哥)吧小刘，出来接客了……. 为什么要写博客我：涛哥我考你一考，你有博客吗涛：不能写罢？……我教给你，记着！这些博客应该记着。将来做架构师的时候，写ppt要用。我：谁要你教，不就写博客么？涛：对呀对呀！……博客有四种写法，你知道么？我愈不耐烦了，努着嘴走远。涛哥见我毫不热心，便又叹一口气，显出极惋惜的样子。 真Verson 2.0-beta建立个人知识体系 初看一些东西，会觉得这个东西可能很简单，但是被人问及后，可能会在很多细节上出现很多漏洞。写博客可以在细节上进行思考，完善自己的知识体系。 写些什么呢 想到啥写啥吧","tags":[]},{"title":"QUIC翻译","date":"2017-04-02T02:57:50.000Z","path":"2017/04/02/QUIC翻译/","text":"QUIC中文翻译翻译地址 翻译过程一些心路历程: 距离上次完整翻译一篇文章已经好几年了，那还是本科毕设用的文章翻译，那时候还能用谷歌翻译，现在也又能了………. 词汇量还是太差。。 对于翻译，并不用把每句话都翻译出来，整段话意思一致就可以了。 对于某些名词，也不需要完完全全翻译。","tags":[{"name":"QUIC","slug":"QUIC","permalink":"http://yoursite.com/tags/QUIC/"},{"name":"传输协议","slug":"传输协议","permalink":"http://yoursite.com/tags/传输协议/"}]},{"title":"linux内核慢启动拥塞避免代码分析","date":"2017-03-27T02:27:29.000Z","path":"2017/03/27/linux内核慢启动拥塞避免代码分析/","text":"@(Network)[tcp, 慢启动] TCP拥塞控制两个速率增长阶段分析0. 参考文档[1] rfc-5681[2] tcp-abc-rfc[3] rfc-3465[4] rfc-3742 1. 拥塞控制个人理解1.1 慢启动与拥塞避免慢启动和拥塞避免，主要是用于拥塞控制中拥塞窗口增长的维护。 根据阈值，拥塞控制其实分为两部分，小于阈值的慢启动阶段，大于阈值进入拥塞避免阶段。 慢启动作为拥塞控制的一部分，我觉得其名字取的比较具有混淆性。个人理解的慢启动分为两种，一种是拥塞窗口小于阈值时候正常的一个指数增长的过程，这个过程中的拥塞窗口不会重置，会持续增长，还有一种是与快速恢复对应的慢启动重新启动，这种时候会将拥塞窗口重置为1，并重新开始指数增长。这么理解的原因如下： 在文档中描述快速恢复时，当收到三个重复ack时候，这时候可能并不是实际丢包，可能是因为链路问题，较晚到达接收端。 在BSD 4.3之前会进入慢启动阶段，但是理论上慢启动一般是指数上升的过程，反而是拥塞避免阶段线性上升速度较慢，且拥塞避免会更新当前的拥塞窗口和阈值，会出现小范围衰减。 假如tcp认为当前包丢失，会很严格的重置拥塞窗口(具体代码如rto触发tcp_enter_loss)，这时候速率曲线不会只是单纯减低到某个值，而是会降低到零点。 1.2 快速恢复和快速重传因此，个人理解，老版本上收到三个重复ack认为丢包，进入丢包处理，重置了拥塞窗口，在非重复ack到来后，拥塞窗口仍然需要从零开始指数上升，而对于快速恢复而言，其只进入拥塞避免阶段，拥塞窗口只是进行一定修正，在非重复ack到来后，仍然能根据阈值来决定是否执行非重启的慢启动，这时候恢复速度相较于严格的丢包处理快了不少。 由于tcp对于丢包的容忍极低，一旦丢包发生，就会进入严格的拥塞处理，而RTO是丢包主要判断依据，因此快速重传也是针对tcp对于丢包容忍度低的一个修正，避免进入RTO，直接影响传输性能。 2. 拥塞控制代码分析本章主要基于reno的拥塞控制。下文中的代码均基于linux kernel 2.6.32版本，直到linux kernel 4.9-rc8之前的版本，tcp整体并没有太大变化。本文不分析frto相关内容。 2.1 调用链由于文档描述上是直接给出一个计算过程，如慢启动阶段的指数上升，和代码直观上看略有不同，因此这里需要先缕清楚整个的调用链，能更好的描述整个拥塞控制的过程。 tcp的拥塞主要是基于定时器(RTO)和ack的，因此主要处理函数都以tcp_ack为起点。这里不分析整个tcp_ack函数，仅分析常规调用链。 整体入口如下： 1234567891011121314151617// 当ack时一个可疑的ack，如sack，或者路由发送的显示拥塞控制，或者当前拥塞状态不是正常状态时。if (tcp_ack_is_dubious(sk, flag)) &#123; /* Advance CWND, if state allows this. */ if ((flag &amp; FLAG_DATA_ACKED) &amp;&amp; !frto_cwnd &amp;&amp; tcp_may_raise_cwnd(sk, flag)) // 当窗口仍然满足可以增长的条件时，进入拥塞控制， // 这是一个钩子函数，具体实现由具体拥塞控制算法来实现， // 对于reno而言可能是慢启动，可能是拥塞避免。 tcp_cong_avoid(sk, ack, prior_in_flight); // 处理拥塞状态机，暂时不展开 tcp_fastretrans_alert(sk, prior_packets - tp-&gt;packets_out, flag);&#125; else &#123; // 当这个ack是一个正常的数据确认包，进入拥塞控制 if ((flag &amp; FLAG_DATA_ACKED) &amp;&amp; !frto_cwnd) tcp_cong_avoid(sk, ack, prior_in_flight);&#125; 2.2 tcp reno的拥塞控制tcp reno注册到拥塞控制框架中的是tcp_reno_cong_avoid函数。 其代码较为简单，只是其中多了一部分tcp-abc的拥塞避免算法，其慢启动实现在tcp_slow_start中，可以参考[rfc-3465][tcp_abc]。大体是用已经确认的byte大小来作为拥塞控制的计算，在慢启动阶段会更加激进，但是可能会带来更大的burst。 1234567891011121314151617181920212223242526272829303132333435/* * TCP Reno congestion control * This is special case used for fallback as well. *//* This is Jacobson's slow start and congestion avoidance. * SIGCOMM '88, p. 328. */void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 in_flight)&#123; struct tcp_sock *tp = tcp_sk(sk); if (!tcp_is_cwnd_limited(sk, in_flight)) return; /* In \"safe\" area, increase. */ // 小于阈值会进入慢启动环节，不重置窗口的慢启动。 if (tp-&gt;snd_cwnd &lt;= tp-&gt;snd_ssthresh) tcp_slow_start(tp); /* In dangerous area, increase slowly. */ else if (sysctl_tcp_abc) &#123; /* RFC3465: Appropriate Byte Count * increase once for each full cwnd acked */ // RFC3465的拥塞避免算法，使用bytes_acked来作为修改拥塞窗口的判断条件 if (tp-&gt;bytes_acked &gt;= tp-&gt;snd_cwnd*tp-&gt;mss_cache) &#123; tp-&gt;bytes_acked -= tp-&gt;snd_cwnd*tp-&gt;mss_cache; if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp) tp-&gt;snd_cwnd++; &#125; &#125; else &#123; // 拥塞避免 tcp_cong_avoid_ai(tp, tp-&gt;snd_cwnd); &#125;&#125; 2.3 慢启动慢启动里面额外涉及两篇rfc，rfc-3742和tcp_abc。 其中snd_cwnd_cnt为线性增长器，只有当线性增长器大于一个窗口大小时，其才会将发送窗口增加，即其单位为1/snd_cwnd，后续还会在拥塞避免代码中见到。 刚开始看代码时对下面那个循环并不是很理解，不理解为什么++是指数增长，直到放到整个调用栈上看，其具体流程如代码注释中所写，为指数增长的过程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/* * Slow start is used when congestion window is less than slow start * threshold. This version implements the basic RFC2581 version * and optionally supports: * RFC3742 Limited Slow Start - growth limited to max_ssthresh * RFC3465 Appropriate Byte Counting - growth limited by bytes acknowledged */void tcp_slow_start(struct tcp_sock *tp)&#123; int cnt; /* increase in packets */ /* RFC3465: ABC Slow start * Increase only after a full MSS of bytes is acked * * TCP sender SHOULD increase cwnd by the number of * previously unacknowledged bytes ACKed by each incoming * acknowledgment, provided the increase is not more than L */ // 不满足tcp abc的窗口增加条件，此时确认的字节数小于mss_cache。 if (sysctl_tcp_abc &amp;&amp; tp-&gt;bytes_acked &lt; tp-&gt;mss_cache) return; // RFC 3742，限制慢启动在一个RTT内的burst。 if (sysctl_tcp_max_ssthresh &gt; 0 &amp;&amp; tp-&gt;snd_cwnd &gt; sysctl_tcp_max_ssthresh) cnt = sysctl_tcp_max_ssthresh &gt;&gt; 1; /* limited slow start */ else // 加上一个窗口大小，在没有abc的情况，保证在最底下的循环中拥塞窗口大小至少增加1. cnt = tp-&gt;snd_cwnd; /* exponential increase */ /* RFC3465: ABC * We MAY increase by 2 if discovered delayed ack */ // tcp-abc，慢启动阶段更激进的burst。 if (sysctl_tcp_abc &gt; 1 &amp;&amp; tp-&gt;bytes_acked &gt;= 2*tp-&gt;mss_cache) cnt &lt;&lt;= 1; tp-&gt;bytes_acked = 0; // 更新snd_cwnd_cnt(窗口线性增长器) tp-&gt;snd_cwnd_cnt += cnt; // 线性增长器是窗口的多少倍，窗口就增加多少。 // 注意：这里的标准场景下的线性增长，每次也只增长1个窗口大小， // 但是其仍然是指数增长，因此每个窗口发出去的数据对应一个ack， // 而每一个ack都会对应触发一次增长。 // 以下为一个简单的例子，sender为发送端，receiver为接收端 // px为包号为x的包，ack x为对第x个包的确认 // snd_cwnd为拥塞窗口 // sender receiver // p1 (snd_cwnd 1) ---------------------------&gt; // // &lt;--------------------------- ack 1 // snd_cwnd++ (2) // // p2 (snd_cwnd 2) ---------------------------&gt; // p3 (snd_cwnd 2) ---------------------------&gt; // // &lt;--------------------------- ack 2 // snd_cwnd++ (3) // &lt;--------------------------- ack 3 // snd_cwnd++ (4) // // p4 (snd_cwnd 4) ---------------------------&gt; // p5 (snd_cwnd 4) ---------------------------&gt; // p6 (snd_cwnd 4) ---------------------------&gt; // p7 (snd_cwnd 4) ---------------------------&gt; // // &lt;--------------------------- ack 4 // snd_cwnd++ (5) // &lt;--------------------------- ack 5 // snd_cwnd++ (6) // &lt;--------------------------- ack 6 // snd_cwnd++ (7) // &lt;--------------------------- ack 7 // snd_cwnd++ (8) // send with snd_cwnd = 8 (p8 - p15) // 每一个ack对应增加一个窗口大小，不丢包的场景下相当于窗口以指数上升 // 1 --&gt; 2 --&gt; 4 --&gt; 8 while (tp-&gt;snd_cwnd_cnt &gt;= tp-&gt;snd_cwnd) &#123; tp-&gt;snd_cwnd_cnt -= tp-&gt;snd_cwnd; if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp) tp-&gt;snd_cwnd++; &#125;&#125; 2.4 拥塞避免拥塞避免的代码比较简短，注意2.3中所写的，snd_cwnd_cnt为线性增长器，其单位为1 / w。在reno调用中，这里的w也为snd_cwnd窗口大小。即每一个ack只增加1 / snd_\\cwnd大小的窗口。 123456789101112/* In theory this is tp-&gt;snd_cwnd += 1 / tp-&gt;snd_cwnd (or alternative w) */void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w)&#123; // 每次cnt++，直到w次后snd_cwnd++，即单位 1 / w if (tp-&gt;snd_cwnd_cnt &gt;= w) &#123; if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp) tp-&gt;snd_cwnd++; tp-&gt;snd_cwnd_cnt = 0; &#125; else &#123; tp-&gt;snd_cwnd_cnt++; &#125;&#125; 3. kernel 4.9的改变对tcp_slow_start的改动不算是4.9的，早在3.18之前就已经改变了，使用的已经不是之前的snd_cwnd_cnt，而是采用tcp-abc算法来进行慢启动。 慢启动仍然使用类似tcp-abc的实现机制，不过其并不以byte作为单位，而是以MSS作为单位进行处理。1234567891011121314151617181920212223/* Slow start is used when congestion window is no greater than the slow start * threshold. We base on RFC2581 and also handle stretch ACKs properly. * We do not implement RFC3465 Appropriate Byte Counting (ABC) per se but * something better;) a packet is only considered (s)acked in its entirety to * defend the ACK attacks described in the RFC. Slow start processes a stretch * ACK of degree N as if N acks of degree 1 are received back to back except * ABC caps N to 2. Slow start exits when cwnd grows over ssthresh and * returns the leftover acks to adjust cwnd in congestion avoidance mode. */u32 tcp_slow_start(struct tcp_sock *tp, u32 acked)&#123; // 使用确认的包数(其中可能包括sack的确认，或者重传数据的确认都加上) // 来更新窗口值，而不是之前的byte。 // 在函数tcp_clean_rtx_queue中有更新对应的delivered。 // 其更新的值貌似和MSS有关系。 u32 cwnd = min(tp-&gt;snd_cwnd + acked, tp-&gt;snd_ssthresh); // 当acked仍然有值，说明超过阈值，处理完slow start后还会进行congestion avoid的处理。 acked -= cwnd - tp-&gt;snd_cwnd; tp-&gt;snd_cwnd = min(cwnd, tp-&gt;snd_cwnd_clamp); return acked;&#125; 拥塞避免上和老版本类似，也使用到了线性增长器，但是涨幅比之前版本较大，并不是以1为计数，而是以acked，即已经确认的MSS个数据片作为单位。 12345678910111213141516171819202122/* In theory this is tp-&gt;snd_cwnd += 1 / tp-&gt;snd_cwnd (or alternative w), * for every packet that was ACKed. */void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w, u32 acked)&#123; /* If credits accumulated at a higher w, apply them gently now. */ // 第一次线性增长计算。 if (tp-&gt;snd_cwnd_cnt &gt;= w) &#123; tp-&gt;snd_cwnd_cnt = 0; tp-&gt;snd_cwnd++; &#125; // 以 acked / snd_cwnd为单位增长。将循环改为除法。 tp-&gt;snd_cwnd_cnt += acked; if (tp-&gt;snd_cwnd_cnt &gt;= w) &#123; u32 delta = tp-&gt;snd_cwnd_cnt / w; tp-&gt;snd_cwnd_cnt -= delta * w; tp-&gt;snd_cwnd += delta; &#125; tp-&gt;snd_cwnd = min(tp-&gt;snd_cwnd, tp-&gt;snd_cwnd_clamp);&#125; @小刘1悦","tags":[{"name":"TCP","slug":"TCP","permalink":"http://yoursite.com/tags/TCP/"},{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"拥塞控制","slug":"拥塞控制","permalink":"http://yoursite.com/tags/拥塞控制/"}]},{"title":"TCP简介-读书笔记","date":"2017-03-19T12:00:38.000Z","path":"2017/03/19/TCP简介-读书笔记/","text":"@(Network)[tcp, congestion control] 读书笔记-TCP简介本文主要记录阅读linuxtcp文章，其第二章中主要介绍了TCP拥塞控制的基础和一些发展历程，这里作为整理。 个人理解，TCP的拥塞分为两部分，一部分是窗口值变化，慢启动和各种拥塞避免算法，这部分只尝试控制发往网络中的包的数量(拥塞窗口)，但是他并不处理是否丢包，是否应该重传，或者快速重传等；另一部分是TCP逐渐支持的一些机制，可以使tcp更好的进行网络状态的估计，不同的网络状态会对第一部分进行反馈，这部分属于框架级别(其实丢包重传和拥塞控制本身并没有完全相关性(比如固定丢包的链路，无线网络等)，但是由于长久以来的拥塞控制是基于丢包的，因此丢包作为网络拥塞状态判断，耦合在TCP的拥塞控制中，而由于丢包对拥塞控制的影响是毁灭性的，因此也在重传上做了较多的处理，来避免因为一些意外的丢包造成的影响，或者是提早避免因为网络拥塞导致的丢包)因此对应的tcp内核中，一部分对应的拥塞控制状态机，这里处理了所有tcp所支持的机制，如SACK,DSACK,FRTO等网络估计的基础，而拥塞避免则会根据猜测的网络状态进行合理控制。 [TOC] 0. 参考资料[1] linux_tcp[2] SACK[3] Duplicate SACK[4] Duplicate SACK ppt[5] FACK[6] FRTO-4138[7] FRTO-5862[8] FRTO-细节[9] An Enhanced Recovery Algorithm for TCP Retransmission Timeouts 1. 慢启动和拥塞避免TCP拥塞控制算法主要由发送端通过拥塞窗口(cwnd)来控制。最开始主要有两个拥塞控制的方法，通过阈值ssthresh来作为两种窗口增长的临界标志。 慢启动：在小于阈值时，当每一个ack到来时，慢启动算法会将拥塞窗口增加一个segment大小。当你有cwnd个包发出，收到cwnd个ack，在慢启动阶段将会增加cwnd个窗口大小，是指数增长的过程。 拥塞避免：当大于阈值后，拥塞避免算法会限制拥塞窗口在一个RTT内只增加一个segment大小。当你有cwnd个包发出，在这个RTT内收到cwnd个ack，在拥塞避免阶段每个ack增加(1 / 拥塞窗口大小)，窗口增加 (1 / cwnd) * cwnd = 1个大小。 2. RTO重传可能被重传定时器RTO触发，RTO一旦超时，表明某个包丢失，对于TCP而言，丢包意味着网络产生拥塞，因此此时会把拥塞窗口降低到最小值(1个segment大小)。因此丢包是非常严苛的拥塞条件，一旦丢包发生，对整个传输效率会造成极大的影响。 In addition, when RTO occurs, the sender resets the congestion window to one segment, since the RTO may indicate that the network load has changed dramatically. This is done because the packet loss is taken as an indication of congestion, and the sender needs to reduce its transmission rate to alleviate the network congestion. 3. RTT与重传二义性由于RTO本身对于TCP性能而言非常严苛，因此RTO使用的定时器时间是否能准确反映网络实际传输情况对于TCP而言非常重要。比如链路RTT突然增加了，但是用于RTO的RTT仍然是之前的小值，这样可能会导致数据虽然没有丢包，但是交往较慢，RTO触发过于频繁，再由于丢包对于TCP的影响，会导致TCP窗口衰减剧烈。 但是TCP对于普通包和重传包使用相同的sequence number，因此当ack到来时，无法区分这个ack对应的是普通包的还是重传包的，因此此时用这个ack的时间戳选项来计算链路RTT可能会导致各种问题。 因此当前使用的RTT计算公式一般是smooth rtt计算。 4. 快速重传和快速恢复由于丢包属于一类重大事故，因此TCP中总是需要尝试提前发现，在其形成重大事故(重置cwnd)前将其提前识别。因此当接收端收到乱序包时，会发送期待的包的序号ack，当发送端收到两个重复的ack时，会发现出现乱序(状态机中的Disorder)，并尝试进行重传来恢复这个包，以免RTO超时触发。当发送端收到三个重复的ack时，会进入快速恢复(状态机中的Recovery)，认为网络可能存在一定的拥塞，会降低拥塞窗口(但是不会像RTO触发以后那样激进)。两种状态下，都会恢复认为丢失或者乱序的包，直到收到非重复的ack为止。 5. 显式拥塞控制由于TCP本身并无法感知到整个网络链路的质量，因此基本是基于自己的算法和丢包反馈来进行拥塞判断，本身存在一定的局限性。 后来的一些路由器在处理数据包，可能可以感知到网络是否真正拥塞，当路由器感知到拥塞时，会通过设置TCP标志位的ECE给TCP，TCP发送端收到带有ECE的ack时会进入拥塞状态(状态机中的CWR)，同时发送一个携带标志位CWR的包给接收端，表示自己当前正在衰减拥塞窗口。 6. Selective acknowledgements由于进入乱序或者快速重传后，在一个RTT之内只能处理一个异常包(不会发送新的包直到新的ack到来，但是当丢包是不连续的若干个，恢复完后仍然会进入对应状态，单独处理下一个乱序丢包，如发送端发送1-6，接收端先收到1，3，6，这时候发送先恢复包2，恢复完包2收到ack发现乱序，请求包4，发送端再恢复4，恢复后发现仍然乱序，请求包5，再次单独恢复包5)，因此在恢复阶段严重影响吞吐量。 SACK并没有打破原有的ack机制，只是在其ack机制上，在TCP option字段中附加了额外信息。4 例子如下图： Multiple packet losses from a window of data can have a catastrophic effect on TCP throughput. SACK does not change the meaning of ACK field. 注: SACK附加在TCP option字段中，option字段最多只有40字节，因此SACK最多包含四个区间。 A SACK option that specifies n blocks will have a length of 8*n+2 bytes, so the 40 bytes available for TCP options can specify a maximum of 4 blocks. 注2: SACK必须接收和发送端都支持才可以正常使用。 7. Duplicate-SACKSACK在rfc 2018定义的时候，并没有声明其收到两个相同的包以后的处理。D-SACK会使接收端发送重复块的信息给发送端。 简单流程如下：当DSACK激活时，最后一个ACK中的SACK第一个区域为重复区域，不同于普通SACK，它是已经收到的区域的一个子区间，每个重复块只会上报一次。 8. Forward AcknowledgementsForward Acknowledgement是基于SACK的相关的拥塞控制算法。 当拥塞发生时，此时已经发生丢包，这时候会引入新的变量fackets_out来统计SACK中的数据量，并根据当前已经发送的数据量una和重传数据量retran来估算本条连接实际在网络中的包数量(总包数 - 已经ACK的数量 + 重传包)。并根据这个数值和拥塞窗口进行比较，如果当前网络中的包数量小于拥塞窗口，说明仍然可以往网络中发送部分数量的包。 这种算法本质上是对网络中的包数量进行更精确的估计，结合DSACK，可以更精准的进行判断，可以在恢复阶段依旧保持一定的速率，在处理乱序包的时候可以比传统TCP更加激进。 9. FRTO当网络链路存在一些突发的特殊场景时，可能会触发超时定时器，由于TCP对于丢包的处理异常严格，可能会造成链路质量下降。 可能的一些场景： 对于移动信号的跨域处理，可能会造成突发的延迟。 对于低带宽链路，偶发的竞争可能也会造成整个RTT的增加。 稳定的链路上可能也有一些原因导致某些包及其重传老是失败。 First, some mobile networking technologies involve sudden delay spikes on transmission because of actions taken during a hand-off. Second, given a low-bandwidth link or some other change in available bandwidth, arrival of competing traffic (possibly with higher priority) can cause a sudden increase of round-trip time. This may trigger a spurious retransmission timeout. A persistently reliable link layer can also cause a sudden delay when a data frame and several retransmissions of it are lost for some reason. 可能造成的一些影响： 虚假超时后的慢启动可能会导致向已经产生拥塞的网络中注入更多的数据包，会严重影响实际网络的拥塞状态。 当虚假超时触发后，可能造成虚假重传，当过多虚假重传发生后，对应的ack回来时可能会触发虚假的快速恢复，如下图，第二次虚假的快速重传是由于第一次虚假RTO超时导致重传发送了重复包导致。 However, if the RTO occurs spuriously and there still are segments outstanding in the network, a false slow start is harmful for the potentially congested network as it injects extra segments to the network at increasing rate. FRTO会在RTO超时后，不会类似传统TCP的超时机制，会额外根据后续两个ACK与当前未确认的最小包进行比较，根据这个结果判断当前RTO是否在安全范围内。如果收到的是未确认的包之后的包，则可能是因为网络原因导致的延迟，可以进入恢复状态。如果收到的是重复的ack，则认为这个包确实已经丢失，进入丢包状态。 10. 概览图简单用图画出之间衍生的关系。","tags":[{"name":"TCP","slug":"TCP","permalink":"http://yoursite.com/tags/TCP/"},{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"拥塞控制","slug":"拥塞控制","permalink":"http://yoursite.com/tags/拥塞控制/"},{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/tags/读书笔记/"}]},{"title":"九层妖塔","date":"2017-03-19T11:18:07.000Z","path":"2017/03/19/九层妖塔/","text":"3只精灵龙的其他故事涛哥分析的好有道理下一期涛哥应该要带来猜奥秘的模型了吧！ 以下代码待验证。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;class tree &#123; public: void build_tree (int dragon, int damage) &#123; vector&lt;int&gt; in; in.push_back (30); for (int i = 1; i &lt; dragon + 1; i++) &#123; in.push_back (2); &#125; build_tree_1 (in, damage, 1.0); &#125; bool result_match (vector&lt;int&gt; &amp;in) &#123; int cnt = 0; for (int i = 1; i &lt; in.size (); i++) if (in[i] != 0) cnt++; return cnt == alive; &#125; void build_tree_1 (vector&lt;int&gt; &amp;in, int damage, float p) &#123; int count = 0; float new_p = 0.0;#ifdef DEBUG for (int i = 0; i &lt; in.size (); i++) cout &lt;&lt; in[i] &lt;&lt; \",\"; cout &lt;&lt; \"Prob \" &lt;&lt; p &lt;&lt; \" else damage \" &lt;&lt; damage &lt;&lt; endl;#endif if (damage == 0) &#123; if (result_match (in)) result += p; return; &#125; for (int i = 0; i &lt; in.size (); i++) &#123; if (in[i] != 0) count++; &#125; if (count == 0) return; new_p = p / count; count = 0; for (int i = 0; i &lt; in.size (); i++) &#123; /* A dead dragon. */ if (in[i] == 0) continue; in[i]--; build_tree_1 (in, damage - 1, new_p); in[i]++; &#125; &#125; tree (int dragon, int damage, int _alive) : alive (_alive) &#123; result = 0.0; build_tree (dragon, damage); &#125; float get_result () &#123; return result; &#125; ~tree () &#123; &#125; private: int alive; float result;&#125;;int main ()&#123; tree t (3, 8, 1); cout &lt;&lt; t.get_result () &lt;&lt; endl; return 0;&#125;","tags":[{"name":"涛哥说的都怼","slug":"涛哥说的都怼","permalink":"http://yoursite.com/tags/涛哥说的都怼/"},{"name":"fun","slug":"fun","permalink":"http://yoursite.com/tags/fun/"}]}]